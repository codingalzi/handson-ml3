
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7. 앙상블 학습과 랜덤 포레스트 &#8212; 핸즈온 머신러닝(3판)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="8. 차원축소" href="dimensionality_reduction.html" />
    <link rel="prev" title="6. 결정트리" href="decision_trees.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">핸즈온 머신러닝(3판)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ml_landscape.html">
   1. 한눈에 보는 머신러닝
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="end2end_ml_project.html">
   2. 머신러닝 프로젝트 처음부터 끝까지
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html">
   3. 분류
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="training_models.html">
   4. 모델 훈련
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svm.html">
   5. 서포트 벡터 머신
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="decision_trees.html">
   6. 결정트리
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   7. 앙상블 학습과 랜덤 포레스트
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dimensionality_reduction.html">
   8. 차원축소
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised_learning.html">
   9. 비지도 학습
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ensemble_learning_random_forests.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/codingalzi/handson-ml3"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/codingalzi/handson-ml3/issues/new?title=Issue%20on%20page%20%2Fensemble_learning_random_forests.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/codingalzi/handson-ml3/master?urlpath=tree/jupyter-book/ensemble_learning_random_forests.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   7.1. 투표식 분류기
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   7.2. 배깅과 페이스팅
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     7.2.1. 사이킷런의 배깅과 페이스팅
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#oob">
     7.2.2. oob 평가
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   7.3. 랜덤 패치와 랜덤 서브스페이스
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   7.4. 랜덤 포레스트
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     7.4.1. 엑스트라 트리
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     7.4.2. 특성 중요도
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   7.5. 부스팅
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     7.5.1. 에이다부스트
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     7.5.2. 그레이디언트 부스팅
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     7.5.3. 히스토그램 그레이디언트 부스팅
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   7.6. 스태킹
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   7.7. 연습문제
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>앙상블 학습과 랜덤 포레스트</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   7.1. 투표식 분류기
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   7.2. 배깅과 페이스팅
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     7.2.1. 사이킷런의 배깅과 페이스팅
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#oob">
     7.2.2. oob 평가
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   7.3. 랜덤 패치와 랜덤 서브스페이스
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   7.4. 랜덤 포레스트
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     7.4.1. 엑스트라 트리
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     7.4.2. 특성 중요도
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   7.5. 부스팅
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     7.5.1. 에이다부스트
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     7.5.2. 그레이디언트 부스팅
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     7.5.3. 히스토그램 그레이디언트 부스팅
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   7.6. 스태킹
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   7.7. 연습문제
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="ch-ensemble">
<span id="id1"></span><h1><span class="section-number">7. </span>앙상블 학습과 랜덤 포레스트<a class="headerlink" href="#ch-ensemble" title="Permalink to this headline">¶</a></h1>
<p><strong>감사의 글</strong></p>
<p>자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다.</p>
<p><strong>소스코드</strong></p>
<p>본문 내용의 일부를 파이썬으로 구현한 내용은
<a class="reference external" href="https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_ensemble_learning_random_forests.ipynb">(구글코랩) 앙상블 학습과 랜덤 포레스트</a>에서
확인할 수 있다.</p>
<p><strong>주요 내용</strong></p>
<p>(1) 편향과 분산의 트레이드오프</p>
<p>앙상블 학습의 핵심은 <strong>편향</strong><font size='2'>bias</font>과
<strong>분산</strong><font size='2'>variance</font>을 줄인 모델을 구현하는 것이다.</p>
<ul class="simple">
<li><p>편향: 예측값과 정답이 떨어져 있는 정도를 나타낸다.
정답에 대한 잘못된 가정으로부터 유발되며
편향이 크면 과소적합이 발생한다.</p></li>
<li><p>분산: 입력 샘플의 작은 변동에 반응하는 정도를 나타낸다.
정답에 대한 너무 복잡한 모델을 설정하는 경우 분산이 커지며,
분산이 크면 과대적합이 발생한다.</p></li>
</ul>
<p>그런데 편향과 분산을 동시에 줄일 수 없다.
이유는 편향과 분산은 서로 트레이드오프 관계를 갖기 때문이다.
예를 들어 회귀 모델의 평균 제곱 오차(MSE)는 편향의 제곱과 분산의 합으로 근사되는데,
회귀 모델의 복잡도에 따른 편향, 분산, 평균 제곱 오차 사이의 관계를
그래프로 나타내면 보통 다음과 같다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/bagging_boosting02.png" width="600"/></div>
<p>&lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">위키백과: 편향-분산 트레이드오프</a>&gt;</p>
<div class="info admonition">
<p class="admonition-title">평균 제곱 오차, 편향, 분산의 관계</p>
<p><a class="reference external" href="http://theanalysisofdata.com/notes/estimators1.pdf">Bias, Variance, and MSE of Estimators</a> 에서
평균 제곱 오차, 분산, 편향 사이의 다음 수학적 관계를 잘 설명한다.</p>
<div class="math notranslate nohighlight">
\[
\text{평균제곱오차} \approx \text{편향}^2 + \text{분산}
\]</div>
</div>
<p>(2) 앙상블 학습</p>
<p><strong>앙상블 학습</strong><font size='2'>ensemble learning</font>은
모델 여러 개를 이용한 훈련과 예측을 진행하는 모델을 구현할 때 사용한다.
결과적으로 분산 또는 편향을 줄이기 사용되며 대표적으로
<strong>배깅</strong><font size='2'>bagging</font> 기법과
<strong>부스팅</strong><font size='2'>boosting</font> 기법이
주로 사용된다.</p>
<ul class="simple">
<li><p>배깅 기법: 독립적으로 학습된 예측기 여러 개의 예측값들의 평균값을 예측값으로
사용하여 분산이 줄어든 모델을 구현한다.</p></li>
<li><p>부스팅 기법: 예측기 여러 개를 순차적으로 쌓아 올려 예측값의 편향를 줄이는
모델을 구현한다.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/bagging_boosting01.png" width="500"/></div><div class="section" id="id2">
<h2><span class="section-number">7.1. </span>투표식 분류기<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>동일한 훈련 세트에 대해 여러 종류의 분류 모델을 이용한 앙상블 학습을 진행한 후에
직접 또는 간접 투표를 통해 예측값을 결정한다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-01.png" width="500"/></div><p><strong>직접 투표</strong></p>
<p>앙상블 학습에 사용된 예측기들의 예측값들 중에서 다수결 방식으로 예측하면
각각의 예측기보다 좋은 성능의 모델을 얻는다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-02.png" width="500"/></div><p><strong>간접 투표</strong></p>
<p>앙상블 학습에 사용된 예측기들의 예측한 확률값들의 평균값으로 예측값 결정한다.
이를 위해서는 모든 예측기가 <code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code> 메서드처럼 확률을 예측하는 기능을 지원해야 한다.
높은 확률에 보다 높은 비중을 두기 때문에 직접 투표 방식보다 성능이 좀 더 좋은 경향이 있다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-04.png" width="500"/></div>
<p>&lt;그림출처: <a class="reference external" href="https://www.kaggle.com/fengdanye/machine-learning-6-basic-ensemble-learning">kaggle</a>&gt;</p>
<p><strong>사이킷런의 투표식 분류기: <code class="docutils literal notranslate"><span class="pre">VotingClassifier</span></code>, <code class="docutils literal notranslate"><span class="pre">VotingRegressor</span></code></strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">voting='hard'</span></code> 또는 <code class="docutils literal notranslate"><span class="pre">voting='soft'</span></code>: 직접 또는 간접 투표 방식 지정 하이퍼파라미터.
기본값은 <code class="docutils literal notranslate"><span class="pre">'hard'</span></code>.</p></li>
<li><p>주의: <code class="docutils literal notranslate"><span class="pre">SVC</span></code> 모델 지정할 때 <code class="docutils literal notranslate"><span class="pre">probability=True</span></code> 사용해야 <code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code> 메서드 지원됨.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">voting_clf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="info admonition">
<p class="admonition-title">투표식 분류 성능 향상의 확률적 근거</p>
<p>이항분포의 누적 분포 함수<font size='2'>cumulative distribution function</font>(cdf)를
이용하여 앙상블 학습의 성능이 향상되는 이유를 설명할 수 있다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="k">def</span> <span class="nf">ensemble_win_proba</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    p: 예측기 하나의 성능.</span>
<span class="sd">    n: 앙상블 크기, 즉 예측기 개수.</span>
<span class="sd">    반환값: 다수결을 따를 때 성공할 확률. 이항 분포의 누적분포함수 활용.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">binom</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="mf">0.4999</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
<p>적중률 51% 모델 1,000개의 다수결을 따르면 74.7% 정도의 적중률 나옴.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ensemble_win_proba</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mf">0.51</span><span class="p">)</span>
<span class="go">0.7467502275561786</span>
</pre></div>
</div>
<p>적중률 51% 모델 10,000개의 다수결을 따르면 97.8% 정도의 적중률 나옴.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ensemble_win_proba</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mf">0.51</span><span class="p">)</span>
<span class="go">0.9777976478701533</span>
</pre></div>
</div>
<p>적중률 80% 모델 10개의 다수결을 따르면 100%에 가까운 성능이 가능함.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ensemble_win_proba</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="go">0.9936306176</span>
</pre></div>
</div>
<p>위 결과는 앙상블 학습에 포함된 각각의 모델이 서로 독립인 것을 전제로한 결과이다.
만약에 훈련에 동일한 데이터를 사용하면 모델 사이의 독립성이 완전히 보장되지 않으며,
경우에 따라 오히려 성능이 하락할 수 있다.
모델들의 독립성을 높이기 위해 매우 다른 알고리즘을 사용하는 다른 종류의
모델을 사용할 수도 있다.</p>
</div>
</div>
<div class="section" id="id3">
<h2><span class="section-number">7.2. </span>배깅과 페이스팅<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>배깅 기법은 여러 개의 동일 모델을 훈련 세트의 다양한 부분집합을
대상으로 학습시키는 방식이다.
부분집합을 임의로 선택할 때의 중복 허용 여부에 따라 앙상블 학습 방식이 달라진다.</p>
<ul class="simple">
<li><p><strong>배깅</strong><font size='2'>bagging</font>: 중복 허용 샘플링(부분집합 선택)</p></li>
<li><p><strong>페이스팅</strong><font size='2'>pasting</font>: 중복 미허용 샘플링(부분집합 선택)</p></li>
</ul>
<div class="info admonition">
<p class="admonition-title">배깅과 부트스트랩</p>
<p>배깅은 bootstrap aggregation의 줄임말이며,
부트스트랩<font size='2'>bootstrap</font>은 전문 통계 용어로 중복허용 리샘플링을 가리킨다.</p>
</div>
<p>아래 그림은 배깅 기법으로 하나의 훈련셋으로 네 개의 동일 예측기를 사용하는 것을 보여준다.
각 예측기의 훈련셋이 다르게, 하지만 중복을 허용하는 방식으로 지정되는 것을 그림에서도 확인할 수 있다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-05.png" width="500"/></div><p><strong>예측값</strong></p>
<p>배깅 또는 페이스팅 모델의 예측값은 분류 모델은 예측값의 최빈값을 선택하며,
회귀 모델은 예측값들의 평균값을 사용한다.</p>
<p><strong>병렬 훈련 및 예측</strong></p>
<p>배깅/페이스팅 모델의 훈련과 예측은 다른 CPU 또는 심지어 다른 컴퓨터 서버를 이용하여 각 모델을 훈련 또는 예측을 하게 만든 후 병합하여 하나의 예측값을 생성하도록 할 수 있다.</p>
<p><strong>편향과 분산</strong></p>
<p>개별 예측기의 경우에 비해 앙상블 모델의 편향은 조금 커지거나 거의 비슷하지만 분산은 줄어든다.
이유는 배깅이 표본 샘플링의 다양성을 보다 많이 추가하기 때문이다.
배깅 방식이 페이스팅 방식보다 과대적합의 위험성일 줄어주기에 기본으로 사용된다.
보다 자세한 설명은
<a class="reference external" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py">Single estimator versus bagging: bias-variance decomposition</a> 을 참고한다.</p>
<div class="section" id="id4">
<h3><span class="section-number">7.2.1. </span>사이킷런의 배깅과 페이스팅<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>사이킷런은 <code class="docutils literal notranslate"><span class="pre">BaggingClassifier</span></code> 분류 모델과 <code class="docutils literal notranslate"><span class="pre">BaggingRegressor</span></code> 회귀 모델을 지원하며
사용법은 다음과 같다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_estimators=500</span></code> 개의 <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code> 모델을 이용항 앙상블 학습.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_samples=100</span></code> 개의 훈련 샘플 사용.</p></li>
<li><p>배깅 방식. 페이스팅 방식을 사용하려면 <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code> 로 지정.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_jobs=-1</span></code> 하이퍼파라미터를 이용하여 사용할 CPU 수 지정. 기본값 -1은 전부 사용을 의미함.</p></li>
<li><p>기본적으로 간전 투표 방식 사용. 하지만 기본 예측기가 <code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code> 메서드를 지원하지 않으면
직접 투표 방식 사용. 결정트리는 <code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code> 메서드를 지원하기에 간접 투표 방식을 사용함.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bag_clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                            <span class="n">max_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>아래 두 이미지는 한 개의 결정트리 모델과 500개의 결정트리 모델의
초승달 데이터셋<font size='2'>moons dataset</font>에 대한 훈련 결과의 차이를 명확하게 보여준다.
배깅을 사용한 오른쪽 결정트리 모델의 일반화 성능이 훨씬 좋음을 알 수 있다.
왼쪽 하나의 결정트리 모델과 비교해서 편향(오류 숫자)은 좀 더 커졌지만
분산(결정 경계의 불규칙성)은 훨씬 덜하다.
하지만 편향이 커졌다는 의미는 각 결정트리들 사이의 연관성이 약해졌음을 의미한다.
즉, 각 결정트리의 독립성이 커졌고, 따라서 학습 모델의 일반화 성능이 좋아졌다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-06.png" width="600"/></div></div>
<div class="section" id="oob">
<h3><span class="section-number">7.2.2. </span>oob 평가<a class="headerlink" href="#oob" title="Permalink to this headline">¶</a></h3>
<p>배깅 기법을 적용하면 모델 훈련에 선택되지 않은 훈련 샘플이 평균적으로 전체 훈련셋의 37% 정도를 차지한다.
이런 샘플을 oob(out-of-bag) 샘플이라 부른다.
oob 평가는 각 샘플에 대해 해당 샘플을 훈련에 사용하지 않은 예측기들로 이루어진 앙상블 모델의 예측값을 이용하여
전체 앙상블 모델의 성능을 검증하는 것이다.</p>
<p><code class="docutils literal notranslate"><span class="pre">BaggingClassifier</span></code> 의 경우 <code class="docutils literal notranslate"><span class="pre">oob_score=True</span></code> 하이퍼파라미터를 사용하면
oob 평가를 자동으로 실행한다.
평가 결과는 <code class="docutils literal notranslate"><span class="pre">oob_score_</span></code> 속성에 저정되며, 테스트 성능과 비슷하게 나온다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bag_clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                            <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>각 샘플에 대한 oob 예측값, 즉 해당 샘플을 훈련에 사용하지 않은 예측기들로 이루어진 앙상블 모델의 예측값은
<code class="docutils literal notranslate"><span class="pre">oob_decision_function_()</span></code> 메서드가 계산한다.
예를 들어, 처음 세 개 훈련 샘플에 대한 oob 예측값은 다음과 같다.
결정트리 모델이 <code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code> 메서드를 지원하기에 양성, 음성 여부를 확률로 계산한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bag_clf</span><span class="o">.</span><span class="n">oob_decision_function_</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
<span class="go">array([[0.32352941, 0.67647059],</span>
<span class="go">       [0.3375    , 0.6625    ],</span>
<span class="go">       [1.        , 0.        ]])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">7.3. </span>랜덤 패치와 랜덤 서브스페이스<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>이미지 데이터의 경우처럼 특성 수가 매우 많은 경우 특성에 대해 중복선택 옵션을 지정할 수 있다.
이를 통해 더 다양한 예측기를 만들며, 편향이 커지지만 분산은 낮아진다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">max_features</span></code> 하이퍼파라미터:
학습에 사용할 특성 수 지정. 기본값은 1.0, 즉 전체 특성 모두 사용.
정수를 지정하면 지정된 수 만큼의 특성 사용.
0과 1 사이의 부동소수점이면 지정된 비율 만큼의 특성 사용.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bootstrap_features</span></code> 하이퍼파라미터:
학습에 사용할 특성을 선택할 때 중복 허용 여부 지정.
기본값은 False. 즉, 중복 허용하지 않음.</p></li>
</ul>
<p><strong>랜덤 패치 기법</strong></p>
<p>훈련 샘플과 훈련 특성 모두를 대상으로 중복을 허용하며 임의의 샘플 수와 임의의 특성 수만큼을 샘플링해서 학습하는 기법이다.</p>
<p><strong>랜덤 서브스페이스 기법</strong></p>
<p>전체 훈련 세트를 학습 대상으로 삼지만 훈련 특성은 임의의 특성 수만큼 샘플링해서 학습하는 기법이다.</p>
<ul class="simple">
<li><p>샘플에 대해: <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>이고 <code class="docutils literal notranslate"><span class="pre">max_samples=1.0</span></code></p></li>
<li><p>특성에 대해: <code class="docutils literal notranslate"><span class="pre">bootstrap_features=True</span></code> 또는 <code class="docutils literal notranslate"><span class="pre">max_features</span></code> 는 1.0 보다 작게.</p></li>
</ul>
</div>
<div class="section" id="id6">
<h2><span class="section-number">7.4. </span>랜덤 포레스트<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p><strong>랜덤 포레스트</strong><font size='2'>random forest</font>는
배깅 기법을 결정트리의 앙상블에 특화시킨 모델이다.
배깅 기법 대신에 페이스팅 기법을 옵션으로 사용할 수도 있으며,
<code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> 는 분류 용도로, <code class="docutils literal notranslate"> <span class="pre">RandomForestRegressor</span></code> 는 회귀 용도로 사용한다.</p>
<p>아래 두 모델은 기본적으로 동일하며, 사용된 하이퍼파라미터는 다음과 같다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_estimators=500</span></code>: 500 개의 결정트리 사용</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_leaf_nodes=16</span></code>: 리프 노드 최대 16개</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_jobs=-1</span></code>: 모든 CPU 사용</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> 모델</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rnd_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                                 <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">BaggingClassifier</span></code> 모델</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bag_clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
    <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">16</span><span class="p">),</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>배깅 모델에 사용된 <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code>의 <code class="docutils literal notranslate"><span class="pre">max_features=&quot;sqrt&quot;</span></code> 하이퍼파라미터 인자는
노드 분할에 사용되는 특성의 수를 전체 특성 개수 <span class="math notranslate nohighlight">\(n\)</span>의 제곱근 값인 <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> 으로 제한한다는 의미다.
더 나아가 특성 선택이 무작위로 이루어진다.
이를 통해 보다 다양한 결정트리를 사용하게 되며, 결과적으로 편향은 좀 더 높지만 보다 좋은 성능의
앙상블 모델이 학습된다.</p>
<p><code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> 모델의 하아퍼파라미터는 <code class="docutils literal notranslate"><span class="pre">BaggingClassifier</span></code>와 <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code>의 옵션을 거의 모두 동일하게 사용한다.</p>
<div class="section" id="id7">
<h3><span class="section-number">7.4.1. </span>엑스트라 트리<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>랜덤 포레스트는 <span class="math notranslate nohighlight">\(\sqrt{n}\)</span> 개의 특성을 무작위로 선택하지만 선택된 특성의 임곗값은 모든 특성값에
대해 확인한다.
그런데 <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code> 모델의 <code class="docutils literal notranslate"><span class="pre">splitter=&quot;random&quot;</span></code> 하이퍼파라미터 인자를 사용하면
임곗값도 무작위로 몇 개 선택해서 그중에 최선의 임곗값을 찾는데,
그런 결정트리로 구성된 앙상블 학습 모델을
<strong>엑스트라 트리</strong><font size='2'>Extra-Tree</font>라 한다.
참고로 엑스트라 트리는 <strong>Extremely Randomized Tree</strong> 의 줄임말이다.</p>
<p>엑스트라 트리는 일반적인 램덤포레스트보다 속도가 훨씬 빠르고,
보다 높은 편향을 갖지만 분산은 상대적으로 낮다.</p>
<p>아래 코드는 사이킷런의 엑스트라 모델을 선언한다.
하이퍼파라미터는 <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code> 를  사용하는 것 이외에는 랜덤포레스트의 경우와 하나만 빼고 동일하다.
<code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code> 를 사용하는 이유는 특성과 임곗값을 무작위로 선택하기에 각
결정트리의 훈련에 사용될 훈련 샘플들까지 중복을 허용해서 모델의 다양성을 굳이
보다 더 키울 필요는 없는 것으로 이해된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">extra_clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
                                 <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>랜덤 포레스트와 엑스트르 트리 두 모델의 성능은 기본적으로 비슷한 것으로 알려졌다.</p>
</div>
<div class="section" id="id8">
<h3><span class="section-number">7.4.2. </span>특성 중요도<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>어떤 특성의 중요도는 해당 특성을 사용한 마디가 평균적으로 불순도를 얼마나 감소시키는지를 측정한 값이다.
즉, 불순도를 많이 줄이면 그만큼 중요도가 커진다.</p>
<p><code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> 모델은 훈련할 때마다 자동으로 모든 특성에 대해
상대적 특성 중요도를 계산하여 <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> 속성에 저장한다.
즉, 모든 특성 중요도의 합은 1이다.
이렇듯 랜덤 포레스트 모델을 이용하여 특성의 상대적 중요도를 파악한 다음에 보다
중요한 특성을 선택해서 활용할 수 있다.</p>
<div class="proof example admonition" id="exp-minist-feature-importance">
<p class="admonition-title"><span class="caption-number">Example 7.1 </span> (붓꽃 데이터셋)</p>
<div class="example-content section" id="proof-content">
<p>붓꽃 데이터셋의 경우 특성별 상대적 중요도는 다음과 같이 꽃잎의 길이와 너비가 매우 중요하며,
꽃받침의 길이와 너비 정보는 상대적으로 훨씬 덜 중요하다.
지금까지 붓꽃 데이터셋을 사용할 때 꽃잎의 길이와 너비 두 개의 특성만을 사용한 이유가 여기에 있다.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>특성</p></th>
<th class="text-align:right head"><p>상대적 중요도</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>꽃받침 길이</p></td>
<td class="text-align:right"><p>0.11</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>곷받침 너비</p></td>
<td class="text-align:right"><p>0.02</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>꽃잎 길이</p></td>
<td class="text-align:right"><p>0.44</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>곷잎 너비</p></td>
<td class="text-align:right"><p>0.42</p></td>
</tr>
</tbody>
</table>
</div>
</div><div class="proof example admonition" id="exp-MNIST-feature-importance">
<p class="admonition-title"><span class="caption-number">Example 7.2 </span> (MNIST)</p>
<div class="example-content section" id="proof-content">
<p>MNIST 데이터셋의 경우 특성으로 사용된 모든 픽셀의 중요도를 그래프로 그리면 다음과 같다.
숫자가 일반적으로 중앙에 위치하였기에 중앙에 위치한 픽셀의 중요도가 보다 높게 나온다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-07.png" width="400"/></div>
</div>
</div></div>
</div>
<div class="section" id="id9">
<h2><span class="section-number">7.5. </span>부스팅<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>약한 성능의 예측기 여러 개를 이용하여 보다 강한 성능의 예측기를 학습시기는 기법이
<strong>부스팅</strong><font size='2'>boosting</font>이다.
일반적으로 여러 개의 예측기를 선형적으로 훈련시키면서 약점을 보완해 나가는 알고리즘을 사용하며
대표적으로 다음 두 기법이 사용된다.</p>
<ul class="simple">
<li><p>에이다부스트<font size='2'>AdaBoost</font></p></li>
<li><p>그레이디언트 부스팅<font size='2'>Gradient Boosting</font></p></li>
</ul>
<p>두 기법은 순차적으로 이전 예측기의 결과를 바탕으로 예측 성능을 조금씩 높혀 간다.
즉, 예측 모델의 편향을 줄여나간다.
하지만 순차적으로 학습하기에 배깅/페이스팅 방식과는 달리 훈련을 동시에 진행할 수 없다는 단점을 갖는다.
훈련 시간이 훨씬 오래 걸릴 수 있고 따라서 훈련셋 또는 특성 수가 너무 커지면
적용이 어려울 수 있다.</p>
<div class="section" id="id10">
<h3><span class="section-number">7.5.1. </span>에이다부스트<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p><strong>에이다부스트</strong><font size='2'>AdaBoost</font> 기법은
하나의 모델을 훈련 시킨 후 <strong>잘못 예측된 샘플을 보다 강조</strong>하면서 해당 모델을 다시 훈련시킨다.
아래 그림은 모델이 제대로 학습하지 못한, 즉 과소적합했던 <strong>샘플에 대한 가중치</strong>를
키우는 방식으로 모델을 다시 훈련시키는 과정을 반복하는 것을 보여준다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-08.png" width="500"/></div><p><strong>샘플 가중치</strong></p>
<p>훈련중에 특정 샘플을 보다 강조하도록 유도하는 값을 <strong>샘플 가중치</strong>라 하며,
모든 훈련 샘플에 대해 지정할 수 있다.
지정하지 않으며 모든 훈련 샘플에 대한 가중치는 동일한 값으로 처리된다.
특정 훈련 샘플에 대한 샘플 가중치를 크게 주면 해당 샘플이 그만큼 더 훈련셋에 많이 포함된 것처럼
처리되어, 해당 샘플의 중요도를 키우게 된다.</p>
<div class="info admonition">
<p class="admonition-title"><code class="docutils literal notranslate"><span class="pre">fit()</span></code> 메서드와 샘플 가중치</p>
<p>사이킷런 모델의 <code class="docutils literal notranslate"><span class="pre">fit()</span></code> 메서드는 <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> 옵션인자를 이용하여
각 훈련 샘플에 대한 가중치를 지정할 수 있다.</p>
</div>
<p><strong>에이다부스트 알고리즘 작동 과정</strong></p>
<p>아래 두 개의 그래프는
초승달 데이터셋에 rbf 커널을 사용하는 SVC 모델을 5번 연속 새로 훈련시킨 결과를 보여준다.
새로 훈련시킬 때마다 이전에 제대로 학습하지 못한 샘플에 대한 샘플 가중치를 키워
해당 샘플에 보다 집중하여 훈련하도록 유도된다.</p>
<p>왼쪽과 오른쪽은 새 모델을 훈련하기 시작할 때 적용할 샘플 가중치의 적용 정도를
지정하는 학습률(<code class="docutils literal notranslate"><span class="pre">learnign_rate</span></code>)을 달리한 결과를 보여준다.</p>
<ul class="simple">
<li><p>왼쪽 그래프: 학습률 1</p></li>
<li><p>오른쪽 그래프: 학습률 0.5</p></li>
</ul>
<p>왼쪽 그래프는 학습률을 너무 크게 잡으면 각 모델의 변화가 커질 수 있음을 보여준다.
이유는 기존의 모델을 새로운 모델로 업데이트할 때 기존 모델로부터 계산된 샘플 가중치를
계산된 그대로 또는 그 이상으로 각 샘플에 적용하면 그만큼 모델의 변화가 커져서
하나의 좋은 모델로 수렴하지 못할 수 있기 때문이다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-09.png" width="600"/></div><div class="info admonition">
<p class="admonition-title">학습률</p>
<p>여기서 말하는 학습률은 <a class="reference internal" href="training_models.html#sec-gradient-descent"><span class="std std-ref">경사하강법의 학습률</span></a>과
다른 개념이지만 모델을 업데이트할 때
샘플 가중치의 적용 정도를 조절한다는 의미에서 비슷한 의미로 사용된다.</p>
</div>
<p><strong>사이킷런의 에이다부스트 모델</strong></p>
<p>사이키런에서 제공하는 에이다부스트 모델은 두 개다.</p>
<ul class="simple">
<li><p>분류 모델: <code class="docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></p></li>
<li><p>회귀 모델: <code class="docutils literal notranslate"><span class="pre">AdaBoostRegressor</span></code></p></li>
</ul>
<p>아래 코드는 결정트리 분류 모델에 에이다부스트 기법을 적용한다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_estimators=30</span></code>: 30번 부스팅 반복</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate=0.5</span></code>: 학습률 0.5</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ada_clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span>
    <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>초승달 데이터셋을 대상으로 훈련시킨 결과는 다음과 같다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-10.png" width="400"/></div></div>
<div class="section" id="id11">
<h3><span class="section-number">7.5.2. </span>그레이디언트 부스팅<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p><strong>그레이디언트 부스팅</strong><font size='2'>Gradient Boosting</font> 기법 또한
이전 모델의 예측에 오차가 있다면 그 오차를 보정하는 새로운 예측기를 새롭게 훈련시킨다.
에이다부스트 기법이 샘플의 가중치를 조정하는 반면에
그레이디언트 부스팅 기법은 이전 예측기에 의해 생성된 <strong>잔차</strong>(residual error)에 대해
새로운 예측기를 학습시킨다.</p>
<div class="info admonition">
<p class="admonition-title">잔차</p>
<p>잔차<font size='2'>residual error</font>는 예측값과 실제값 사이의 오차를 가리킨다.</p>
</div>
<p><strong>사이킷런 그레이디언트 부스팅 모델</strong></p>
<p>사이키런에서 제공하는 그레이디언트 부스팅 모델은 두 개다.</p>
<ul class="simple">
<li><p>분류 모델: <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></p></li>
<li><p>회귀 모델: <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></p></li>
</ul>
<p>두 모델 모두 결정트리 모델을 연속적으로 훈련시킨다.</p>
<p><strong>예제: GBRT</strong></p>
<p>아래 그래프는 2차 다항식 모양의 훈련 데이터셋에 결정트리 모델을 3번 연속 적용하면서
생성한 예측값의 변화과정을 보여준다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-11.png" width="700"/></div><p>위 그래프는 아래 <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> 모델을 훈련할 때 실제로 학습되는 과정을 잘 보여준다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<div class="info admonition">
<p class="admonition-title">GBRT</p>
<p>GBRT는 Gradient Boosted Regression Trees, 즉 ‘그레이디언트 부스팅 회귀 나무’의 줄임말이다.</p>
</div>
<p><strong>학습률과 축소 규제</strong></p>
<p>학습률(<code class="docutils literal notranslate"><span class="pre">learnign_rate</span></code>)는 그레이디언트 부스팅 기법으로 훈련된 모델의
예측값을 정할 때 훈련 과정에 사용된 각 모델의 예측값이 기여하는 정도를 결정한다.</p>
<p>학습률이 0.1 등처럼 작게 잡으면 보다 많은 수의 모델을 훈련시켜야 하지만
그만큼 일반화 성능이 좋은 모델이 훈련된다.
이런 방식으로 훈련 과정을 규제하는 기법을 <strong>축소 규제</strong><font size='2'>shrinkage regularization</font>다.
즉, 훈련에 사용되는 각 모델의 기여도를 어느 정도로 축소할지 결정하는 방식으로
모델의 훈련을 규제한다는 의미다.</p>
<p>아래 두 그래프는 학습률이 1인 경우(왼쪽)와 0.05인 경우(오른쪽)의 차이를 보여준다.
학습률이 1인 경우 모델 훈련을 세 번만 반복해서 과소적합이 발생했지만,
학습률이 0.05인 경우 모델 훈련을 92번 반복해서 적절한 모델을 훈련시켰다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-12a.png" width="700"/></div><p><strong>조기 종료</strong></p>
<p>훈련 모델의 수를 너무 크게 잡으면 과대적합의 위험성은 커지게 된다.
따라서 훈련되는 모델의 적절한 수를 알아내는 일이 중요하다.
이를 위해 그리드 탐색 기법, 랜덤 탐색 기법 등을 사용할 수 있다.</p>
<p>하지만 <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> 모델의 <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> 하이퍼파라미터를 지정하면
간단하게 <a class="reference internal" href="training_models.html#sec-early-stopping"><span class="std std-ref">조기 종료</span></a> 기법을 적용할 수 있다.</p>
<p>아래 코드는 <code class="docutils literal notranslate"><span class="pre">n_iter_no_change=10</span></code> 을 사용하여 모델이 연속적으로 10번 제대로 개선되지 못하는 경우
훈련을 종료시킨다.
원래 500 번 연속 결정트리를 훈련시켜야 하지만 실제로는 훨씬 일찍 훈련을 종료한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gbrt_best</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>확률적 그레이디언트 부스팅</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">subsample</span></code> 하이퍼파라리미터를 이용하여 각 결정트리가 훈련에 사용할 훈련 샘플의 비율을 지정한다.
예를 들어 <code class="docutils literal notranslate"><span class="pre">subsample=0.25</span></code> 로 설정하면 각 결정트리 모델은 전체 훈련셋의 25% 정도만
이용해서 훈련한다. 훈련 샘플은 매번 무작위로 선택된다.
이 방식을 사용하면 훈련 속도가 빨라지며, 편향은 높아지지만, 모델의 다양성이 많아지기에 분산은 낮아진다.</p>
</div>
<div class="section" id="id12">
<h3><span class="section-number">7.5.3. </span>히스토그램 그레이디언트 부스팅<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>대용량 데이터셋을 이용하여 훈련해야 하는 경우
<strong>히스토그램 그레이디언트 부스팅</strong><font size='2'>Histogram-based Gradient Boosing</font>(HGB)
기법이 추천된다.
이 기법은 훈련 샘플의 특성값을 최대 255개의 구간으로 분류한다.
즉, 샘플의 특성이 최대 255개의 값 중에 하나라는 의미다.
원래는 훈련 샘플 수만큼 다른 특성값이 존재한다.</p>
<p>이렇게 하면 결정트리의 CART 알고리즘이 적용될 때 최적의 임곗값을 결정할 때
확인해야 하는 경우의 수가 매우 작아지기에 수 백배 이상 빠르게 학습된다.
또한 특성값이 모두 정수이기에 메모리도 보다 효율적으로 사용한다.
실제로 하나의 결정트리 모델의 훈련 시간 복잡도는 원래 <span class="math notranslate nohighlight">\(O(n\times m \times \log(m))\)</span> 이지만
히스토그램 방식을 사용하면 <span class="math notranslate nohighlight">\(O(n\times m)\)</span> 로 줄어든다.
이유는 특성값을 정렬할 필요가 없어지기 때문이다.
물론 모델의 정확도는 떨어진다. 하지만 경우에 따라 과대적합을 방지하는 규제 역할도 수행한다.</p>
<p><strong>사이킷런의 히스토그램 그레이디언트 부스팅 모델</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code>: 회귀 모델</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code>: 분류 모델</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>, <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> 등과 유사하게 작동한다.</p>
<p><strong>장점</strong></p>
<p>언급된 두 모델은 범주 특성을 다룰 수 있으며, 결측치도 처리할 수 있다.
결측치는 255개의 구간 이외에 특별한 구간에 들어가는 것으로 간주된다.</p>
<p><strong>예제</strong></p>
<p>아래 코드는 <a class="reference internal" href="end2end_ml_project.html#ch-end2end"><span class="std std-ref">캘리포니아 주택가격 데이터셋</span></a>을 이용하여
히스토그램 그레이디언트 부스팅 모델을 적용하는 것을 보여준다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">(OrdinalEncoder(),</span> <span class="pre">[&quot;ocean_proximity&quot;])</span></code> : 해안 근접도 특성값으로 사용된 5개를 0에서 4사이의 정수로 변환하는 변환기 지정.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">categorical_features=[0]</span></code> : 범주형 특성의 위치 지정</p></li>
<li><p>캘리포니아 주택가격 데이터셋에 결측치 존재하지만 전처리로 다루지 않음.</p></li>
<li><p>스케일링, 원-핫-인코딩 등도 필요하지 않음.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hgb_reg</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">make_column_transformer</span><span class="p">((</span><span class="n">OrdinalEncoder</span><span class="p">(),</span> <span class="p">[</span><span class="s2">&quot;ocean_proximity&quot;</span><span class="p">]),</span>
                            <span class="n">remainder</span><span class="o">=</span><span class="s2">&quot;passthrough&quot;</span><span class="p">),</span>
    <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">hgb_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">housing</span><span class="p">,</span> <span class="n">housing_labels</span><span class="p">)</span>
</pre></div>
</div>
<div class="tip admonition">
<p class="admonition-title">기타 그레이디언트 부스팅 모델</p>
<p>다음 모델들이 제3자 라이브러리로 제공되지만 사이킷런의 모델과 유사하게 작동한다.</p>
<ul class="simple">
<li><p>XGBoost</p></li>
<li><p>CatBoost</p></li>
<li><p>LightGBM</p></li>
</ul>
<p>또한
<a class="reference external" href="https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras">텐서플로우<font size='2'>Tensorflow</font>의 랜덤 포레스트와 GBRT 관련 다양한 최신 모델</a>도 있다.</p>
</div>
</div>
</div>
<div class="section" id="id13">
<h2><span class="section-number">7.6. </span>스태킹<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<p>배깅방식의 응용으로 볼 수 있는 기법이다.
다수결을 이용하는 대신 여러 예측값을 훈련 데이터로 활용하는 예측기를 훈련시킨다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-13.png" width="400"/></div><p><strong>사이킷런의 <code class="docutils literal notranslate"><span class="pre">StackingRegressor</span></code> 모델 활용법</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
              <span class="p">(</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
              <span class="p">(</span><span class="s1">&#39;knr&#39;</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                                          <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))]</span>

<span class="n">final_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
                        <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span>
                        <span class="n">final_estimator</span><span class="o">=</span><span class="n">final_estimator</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>사이킷런의 <code class="docutils literal notranslate"><span class="pre">StackingClassifier</span></code> 모델 활용법</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
              <span class="p">(</span><span class="s1">&#39;svr&#39;</span><span class="p">,</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                                    <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)))]</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span> 
                         <span class="n">final_estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>스태킹 모델의 예측값</strong></p>
<p>레이어를 차례대로 실행해서 믹서기(블렌더)가 예측한 값을 예측값으로 지정한다.
훈련된 스태킹 모델의 편향과 분산이 훈련에 사용된 모델들에 비해 모두 감소한다.</p>
<p><strong>다층 스태킹</strong></p>
<p>2층에서 여러 개의 믹서기(블렌더)를 사용하고,
그위 3층에 새로운 믹서기를 추가하는 방식으로 다층 스태킹을 훈련시킬 수 있다.
다층 스태킹의 훈련 방식은 2층 스태킹의 훈련 방식을 반복하면 된다.</p>
<p><strong>예제: 3층 스태킹 모델 훈련과정</strong></p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch07/homl07-17.png" width="400"/></div></div>
<div class="section" id="id14">
<h2><span class="section-number">7.7. </span>연습문제<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>참고: <a class="reference external" href="https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/practices/practice_ensemble_learning_random_forests.ipynb">(실습) 앙상블 학습과 랜덤 포레스트</a></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="decision_trees.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">6. </span>결정트리</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="dimensionality_reduction.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8. </span>차원축소</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By 코딩알지<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>