{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch:svm)=\n",
    "# 서포트 벡터 머신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**감사의 글**\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**소스코드**\n",
    "\n",
    "본문 내용의 일부를 파이썬으로 구현한 내용은 \n",
    "[(구글코랩) 모델 훈련](https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_training_models.ipynb)에서 \n",
    "확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주요 내용**\n",
    "\n",
    "* 선형 SVM 분류\n",
    "* 비선형 SVM 분류\n",
    "* SVM 회귀\n",
    "* SVM 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**목표**\n",
    "\n",
    "수 백에서 수 천개의 데이터로 구성된 작은 훈련 데이터셋을 대상으로 선형과 비선형 분류 모델을\n",
    "매우 잘 훈련시키는 서포트 벡터 머신의 주요 개념, 사용법, 작동법을 알아본다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 SVM 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 **서포트 벡터 머신**<font size=\"2\">support vector machine</font>(SVM)은\n",
    "두 클래스 사이를 최대한으로 경계 도로를 최대한 넓게 잡으려고 시도한다. \n",
    "이때 두 클래스 사이에 놓을 수 있는 결정 경계 도로의 폭의 **마진**<font size='2'>margin</font>이라 하며,\n",
    "마진을 최대로 하는 분류가 **큰 마진 분류**<font size='2'>large maring classication</font>이다.\n",
    "\n",
    "아래 그림은 붓꽃 데이터셋을 대상으로 해서 선형 분류와 큰 마진 분류의 차이점을 보여준다.\n",
    "선형 분류(왼쪽 그래프)의 경우 두 클래스를 분류하기만 해도 되는 반면에 큰 마진 분류(오른쪽 그래프)의 \n",
    "결정 경계(검은 실선)는 두 클래스와 거리를 최대한 크게 두려는 방향으로 정해진다.\n",
    "즉, 마진은 가능한 최대로 유지하려 한다. \n",
    "큰 마진 분류의 결정 경계는 결정 경계 도로의 가장자리에 위치한\n",
    "**서포트 벡터**<font size='2'>support vector</font>에만 의존하며 다른 데이터와는 전혀 상관 없다.\n",
    "아래 오른쪽 그래프에서 서포트 벡터는 동그라미로 감싸져 있다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-01.png\" width=\"700\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} 스케일링과 마진\n",
    ":class: info\n",
    "\n",
    "특성의 스케일을 조정하면 결정 경계가 훨씬 좋아진다. \n",
    "두 특성의 스케일에 차이가 많이 나는 경우(아래 왼쪽 그래프) 보다\n",
    "표준화된 특성을 사용할 때(아래 오른쪽 그래프) 훨씬 좋은 결정 경계가 찾아진다. \n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-02.png\" width=\"700\"/></div>\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하드 마진 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 훈련 샘플이 도로 바깥쪽에 올바르게 분류되도록 하는 마진 분류가\n",
    "**하드 마진 분류**<font size='2'>hard margin classification</font>이다. \n",
    "하지만 두 클래스가 선형적으로 구분되는 경우에만 적용 가능하다. \n",
    "\n",
    "또한 이상치에 매우 민감하다.\n",
    "하나의 이상치가 추가되면 선형 분류가 불가능하거나(아래 왼편 그래프)\n",
    "일반화가 매우 어려운 분류 모델(아래 오른편 그래프)이 얻어질 수 있다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-03.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트 마진 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**소프트 마진 분류**<font size='2'>soft margin classification</font>는 어느 정도의 마진 오류를 허용하면서\n",
    "결정 경계 도로의 폭을 최대로 하는 방향으로 유도한다.\n",
    "**마진 오류**<font size='2'>margin violations</font>는 결정 경계 도로 상에 또는 결정 경계를 넘어 해당 클래스 반대편에 위치하는 샘플을 의미한다. \n",
    "\n",
    "예를 들어 꽃잎 길이와 너비 기준으로 붓꽃의 버지니카와 버시컬러 품종을 하드 마진 분류하기는 불가능하며,\n",
    "아래 그래프에서처럼 어느 정도의 마진 오류를 허용해야 한다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-03b.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`LinearSVC` 클래스**\n",
    "\n",
    "사이킷런의 `LinearSVC` 클래스는 선형 SVM 분류기를 생성한다.\n",
    "\n",
    "```python\n",
    "LinearSVC(C=1, random_state=42)\n",
    "```\n",
    "\n",
    "`C` 는 규제 강조를 지정하는 하이퍼파라미터이며 클 수록 적은 규제를 의미한다. \n",
    "`C` 가 너무 작으면(아래 왼편 그래프) 마진 오류를 너무 많이 허용하는 과소 적합이\n",
    "발생하며, `C` 를 키우면(아래 오른편 그래프) 결정 경계 도로 폭이 좁아진다.\n",
    "여기서는 `C=100` 이 일반화 성능이 좋은 모델을 유도하는 것으로 보인다.\n",
    "또한 `C=float(\"inf\")`로 지정하면 하드 마진 분류 모델이 된다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-04.png\" width=\"800\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} 선형 SVM 지원 모델\n",
    ":class: info\n",
    "\n",
    "`LinearSVC` 모델은 대용량 훈련 데이터셋을 이용해서도 빠르게 학습한다. \n",
    "이외에 `SVC` 모델과 `SGDClassifier` 모델도 선형 SVM 분류 모델로 활용될 수 있다.\n",
    "\n",
    "* `SVC` 클래스 활용\n",
    "\n",
    "    ```python\n",
    "    SVC(kernel=\"linear\", C=1)\n",
    "    ```\n",
    "\n",
    "* `SGDClassifier` 클래스 활용\n",
    "    \n",
    "    ```python\n",
    "    SGDClassifier(loss=\"hinge\", alpha=1/(m*C))\n",
    "    ```\n",
    "\n",
    "hinge 손실 함수는 어긋난 예측 정도에 비례하여 손실값이 선형적으로 커진다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-06c.png\" width=\"400\"/></div>\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비선형 SVM 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형적으로 구분되지 못하는 데이터셋을 대상으로 분류 모델을 훈련시키는 두 가지 방식을 소개한다.\n",
    "\n",
    "* 방식 1: 특성 추가 + 선형 SVC\n",
    "    * 다항 특성 활용: 다항 특성을 추가한 후 선형 SVC 적용\n",
    "    * 유사도 특성 활용: 유사도 특성을 추가한 후 선형 SVC 적용\n",
    "\n",
    "* 방식 2: `SVC` + 커널 트릭\n",
    "    * 커널 트릭: 새로운 특성을 실제로 추가하지 않으면서 동일한 결과를 유도하는 방식\n",
    "    * 예제 1: 다항 커널\n",
    "    * 예제 2: 가우시안 RBF(방사 기저 함수) 커널"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**다항 특성 추가 + 선형 SVM**\n",
    "\n",
    "{numref}`%s절 <sec:poly_reg>`에서 설명한 다항 회귀 기법에서 다항 특성을 추가한 후에 \n",
    "선형 회귀를 적용한 방식과 동일하다. \n",
    "아래 그래프는 특성 $x_1$ 하나만 갖는 데이터셋에 특성 $x_1^2$을 추가한 후 선형 회귀 모델을\n",
    "적용한 결과를 보여준다.\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\, x_1 + \\theta_2\\, x_1^{2}$$\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-07.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동일한 아이디어를 특성 $x_1$ 하나만 갖는 데이터셋(아래 왼편 그래프)에 적용하면 \n",
    "비선형 SVM 모델(아래 오른편 그래프)을 얻게 된다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-05.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} 2차 다항 특성 추가 후 선형 SVM 분류 모델 훈련\n",
    ":class: info\n",
    "\n",
    "아래 동영상은 두 개의 특성을 갖는 데이터셋에 2차 다항 특성을 추가한 후에 선형 SVM 분류 모델을\n",
    "적용하는 과정을 보여준다. \n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/svm-poly-kernel.gif\" width=\"600\"/></div>\n",
    "\n",
    "<동영상 출처: [SVM with polynomial kernel visualization](https://www.youtube.com/watch?v=OdlNM96sHio)>\n",
    "\n",
    "참고로 3차원 상에서의 선형 방정식의 그래프는 평면으로 그려진다. \n",
    "\n",
    "$$z = \\frac{3}{5} x + \\frac{1}{5}y + 5 \\quad\\Longleftrightarrow\\quad 3x + y - 5z + 25 = 0$$\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-06d.png\" width=\"500\"/></div>\n",
    "\n",
    "<그림 출처: [지오지브라(GeoGebra)](https://www.geogebra.org/3d)>\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{prf:example} moons 데이터셋\n",
    ":label: exp:moons_dataset\n",
    "\n",
    "moons 데이터셋은 마주보는 두 개의 반원 모양의 클래스로 구분되는 데이터셋을 가리킨다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-06.png\" width=\"500\"/></div>\n",
    "\n",
    "위 데이터셋에 선형 SVM 분류 모데를 적용하기 위해 먼저 3차 항에 해당하는 특성을 추가하면\n",
    "비선형 분류 모델을 얻게 된다.\n",
    "\n",
    "```python\n",
    "# 3차 항까지 추가\n",
    "polynomial_svm_clf = make_pipeline(\n",
    "    PolynomialFeatures(degree=3),\n",
    "    StandardScaler(),\n",
    "    LinearSVC(C=10, max_iter=10_000, random_state=42)\n",
    ")\n",
    "```\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-07.png\" width=\"500\"/></div>\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다항 커널"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다항 특성을 추가하는 기법은 그만큼 비용을 지불해야 한다.\n",
    "특히 축가해야 하는 특성이 많다면 시간과 메모리 사용 비용이 엄청날 수 있다.\n",
    "반면에 **커널 트릭**<font size='2'>kernel trick</font>을 사용하면\n",
    "다항 특성을 실제로는 추가하지 않지만 추가한 경우와 동일한 결과를 만들어 낼 수 있다.\n",
    "다만 이것은 SVM을 적용하는 경우에만 해당한다.\n",
    "이와 달리 다항 특성을 추가하는 기법은 어떤 모델과도 함께 사용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 두 그래프는 커널 기법을 사용하는 SVC 모델을 moons 데이터셋에 대해 훈련시킨 결과를 보여준다.\n",
    "\n",
    "```python\n",
    "poly_kernel_svm_clf = make_pipeline(StandardScaler(),\n",
    "                                    SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "```\n",
    "\n",
    "위 코드는 3차 다항 커널을 적용한 모델이며 아래 왼편 그래프와 같은 분류 모델을 학습한다.\n",
    "반면에 아래 오른편 그래프는 10차 다항 커널을 적용한 모델이다. \n",
    "`coef0` 하이퍼파라미터는 고차항의 중요도를 지정하며, 아래 그래프에서는 $r$ 이 동일한 \n",
    "하이퍼파라미터를 가리킨다.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-09.png\" width=\"800\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} 하이퍼파라미터 이해의 중요성\n",
    ":class: tip\n",
    "\n",
    "다항 커널 모델이 과대 적합이면 차수를 줄여야 하고, 과소 적합이면 차수를 늘려야 한다.\n",
    "적절한 하이퍼파라미터는 그리드 탐색 등을 이용하여 찾으면 되지만,\n",
    "그럼에도 불구하고 하이퍼파라미터의 의미를 잘 알고 있으면 탐색 구간을 줄일 수 있다.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사도 특성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**유사도 함수**\n",
    "\n",
    "* 유사도 함수: __랜드마크__(landmark)라는 특정 샘플과 각 샘플 사이의 유사도(similarity)를 측정하는 함수\n",
    "\n",
    "* 유사도 함수 예제: __가우시안 방사 기저 함수__(RBF, radial basis function)\n",
    "\n",
    "    $$\n",
    "    \\phi(\\mathbf x, \\ell) = \\exp(-\\gamma\\, \\lVert \\mathbf x - \\ell \\lVert^2)\n",
    "    $$\n",
    "\n",
    "    * $\\ell$: 랜드마크\n",
    "    * $\\gamma$: 랜드마크에서 멀어질 수록 0에 수렴하는 속도를 조절함\n",
    "    * $\\gamma$ 값이 클수록 가까운 샘플 선호, 즉 샘플들 사이의 영향을 보다 적게 고려하여\n",
    "        모델의 자유도를 높이게 되어 과대적합 위험 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예제\n",
    "\n",
    "$$\n",
    "\\exp(-5\\, \\lVert \\mathbf x - 1 \\lVert^2) \\qquad\\qquad\\qquad \\exp(-100\\, \\lVert \\mathbf x - 1 \\lVert^2)\n",
    "$$\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-08b.png\" width=\"1200\"/></div>\n",
    "\n",
    "<그림 출처: [데스모스(desmos)](https://www.desmos.com/calculator?lang=ko)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**유사도 특성 추가 + 선형 SVC**\n",
    "\n",
    "* 모든 샘플을 랜드마크로 지정 후 각 랜드마크에 대한 유사도를 새로운 특성으로 추가하는 방식이 가장 간단함.\n",
    "\n",
    "* ($n$ 개의 특성을 가진 $m$ 개의 샘플) $\\Rightarrow$ ($n + m$ 개의 특성을 가진 $m$ 개의 샘플)\n",
    "\n",
    "* 장점: 차원이 커지면서 선형적으로 구분될 가능성이 높아짐.\n",
    "\n",
    "* 단점: 훈련 세트가 매우 클 경우 동일한 크기의 아주 많은 특성이 생성됨.\n",
    "\n",
    "* 예제\n",
    "    * 랜드마크: -2와 1\n",
    "    * $x_2$와 $x_3$: 각각 -2와 1에 대한 가우시안 RBF 함수로 계산한 유사도 특성\n",
    "    * 화살표가 가리키는 점: $\\mathbf x = -1$\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-08.png\" width=\"800\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가우시안 RBF 커널"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM 모델을 훈련시킬 때 유사도 특성을 실제로는 추가 하지 않으면서 수학적으로는 추가한 효과를 내는 성질 이용\n",
    "\n",
    "```python\n",
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=0.1, C=0.001)) ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC + RBF 커널 예제: moons 데이터셋**\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-10.png\" width=\"600\"/></div>\n",
    "\n",
    "\n",
    "|      | 상단 그래프      | 하단 그래프    |\n",
    "| :--- | :------------- | :------------- |\n",
    "| gamma | 랜드마크에 조금 집중 | 랜드마크에 많이 집중 |\n",
    "\n",
    "|      | 왼편 그래프&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 오른편 그래프    |\n",
    "| :--- | :------------- | :------------- |\n",
    "| C | 규제 많이 | 규제 적게 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**추천 커널**\n",
    "\n",
    "* `SVC`의 `kernel` 기본값은 `\"rbf\"` => 대부분의 경우 이 커널이 잘 맞음\n",
    "\n",
    "* 선형 모델이 예상되는 경우 `SVC`의 `\"linear\"` 커널을 사용할 수 있음\n",
    "    하지만 훈련 세트가 크거나 특성이 아주 많을 경우 `LinearSVC`가 빠름\n",
    "\n",
    "* 시간과 컴퓨팅 성능이 허락한다면 교차 검증, 그리드 탐색을 이용하여 적절한 커널을 찾아볼 수 있음\n",
    "\n",
    "* 훈련 세트에 특화된 커널이 알려져 있다면 해당 커널을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계산 복잡도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기|시간 복잡도(m 샘플 수, n 특성 수)|외부 메모리 학습|스케일 조정|커널 트릭|다중 클래스 분류\n",
    "----|-----|-----|-----|-----|-----\n",
    "LinearSVC | $O(m \\times n)$ | 미지원 | 필요 | 미지원 | OvR 기본\n",
    "SGDClassifier | $O(m \\times n)$ | 지원 | 필요 | 미지원 | 지원\n",
    "SVC | $O(m^2 \\times n) \\sim O(m^3 \\times n)$ | 미지원 | 필요 | 지원 | OvR 기본"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1lYLawkuMlw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SVM 분류 vs. SVM 회귀**\n",
    "\n",
    "* SVM 분류 \n",
    "    - 목표: 마진 오류 발생 정도를 조절(`C` 이용)하면서 두 클래스 사이의 도로폭을 최대한 넓게 하기\n",
    "    - 마진 오류: 도로 위에 위치한 샘플\n",
    "\n",
    "* SVM 회귀 \n",
    "    - 목표: 마진 오류 발생 정도를 조절(`C` 이용)하면서 지정된 폭의 도로 안에 가능한 많은 샘플 포함하기\n",
    "    - 마진 오류: 도로 밖에 위치한 샘플\n",
    "    - 참고: [MathWorks: SVM 회귀 이해하기](https://kr.mathworks.com/help/stats/understanding-support-vector-machine-regression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU8IiHfouli_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 SVM 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU8IiHfouli_",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 선형 회귀 모델을 SVM을 이용하여 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU8IiHfouli_",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 예제: LinearSVR 활용. `epsilon`은 도로폭 결정\n",
    "\n",
    "    ```python\n",
    "    from sklearn.svm import LinearSVR\n",
    "    svm_reg = LinearSVR(epsilon=1.5)\n",
    "    ```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iU8IiHfouli_",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 마진 안, 즉 결정 경계 도로 위에 포함되는 샘플를 추가해도 예측에 영향 주지 않음. 즉 `epsilon`에 둔감함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-11.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVTdpLgOvuGf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 비선형 SVM 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVTdpLgOvuGf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* SVC와 동일한 커널 트릭을 활용하여 비선형 회귀 모델 구현\n",
    "\n",
    "* 예제: SVR + 다항 커널\n",
    "\n",
    "    ```python\n",
    "    # SVR + 다항 커널\n",
    "    from sklearn.svm import SVR\n",
    "\n",
    "    svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "    ```\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-12.png\" width=\"800\"/></div>\n",
    "\n",
    "\n",
    "| 왼편 그래프(C=100)    | 오른편 그래프(C=0.01)    |\n",
    "| -------------: | -------------: |\n",
    "| 규제 보다 약함 | 규제 보다 강함 |\n",
    "| 샘플에 덜 민감 | 샘플에 더 민감 |\n",
    "| 마진 오류 보다 적게 | 마진 오류 보다 많이  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FPyRJPJws_I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**회귀 모델 시간 복잡도**\n",
    "\n",
    "* `LinearSVR`: `LinearSVC`의 회귀 버전\n",
    "    * 시간 복잡도가 훈련 세트의 크기에 비례해서 선형적으로 증가\n",
    "\n",
    "* `SVR`: `SVC`의 회귀 버전\n",
    "    * 훈련 세트가 커지면 매우 느려짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVM 분류기의 결정 함수, 예측, 결정 경계, 목적함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**결정 함수와 예측**\n",
    "\n",
    "* 결정 함수: 아래 값을 이용하여 클래스 분류\n",
    "\n",
    "$$\n",
    "h(\\mathbf x) = \\mathbf w^T \\mathbf x + b = w_1 x_1 + \\cdots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "* 예측값: 결정 함수의 값이 양수이면 양성, 음수이면 음성으로 분류\n",
    "\n",
    "$$\n",
    "\\hat y = \\begin{cases}\n",
    "            0 & \\text{if } h(\\mathbf x) < 0\\\\\n",
    "            1 & \\text{if } h(\\mathbf x) \\ge 0\n",
    "         \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**결정 경계**\n",
    "\n",
    "* 결정 경계: 결정 함수의 값이 0인 점들의 집합\n",
    "\n",
    "$$\\{\\mathbf x \\mid h(\\mathbf x)=0  \\}$$\n",
    "\n",
    "* 결정 경계 도로의 경계: 결정 함수의 값이 1 또는 -1인 샘플들의 집합\n",
    "\n",
    "$$\\{\\mathbf{x} \\mid h(\\mathbf x)= \\pm 1 \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**예제**\n",
    "\n",
    "붓꽃 분류. 꽃잎 길이와 너비를 기준으로 버지니카(Iris-Virginica, 초록 삼각형) 품종 여부 판단\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-13.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**결정 함수의 기울기**\n",
    "\n",
    "* 결정 경계면(결정 함수의 그래프, 하이퍼플레인)의 기울기가 작아질 수록 도로 경계 폭이 커짐.\n",
    "\n",
    "* 결정 경계면 기울기가 $\\| \\mathbf w \\|$에 비례함. \n",
    "    따라서 결정 경계 도로의 폭을 크게 하기 위해 $\\| \\mathbf w \\|$를 최소화해야 함.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch05/homl05-14.png\" width=\"600\"/></div>\n",
    "\n",
    "* 하드 마진 모델 훈련: 모든 양성(음성) 샘플이 결정 경계 도로 밖에 위치하도록 하는 기울기 찾기.\n",
    "\n",
    "* 소프트 마진 모델 훈련: 결정 경계 도로 위에 위치하는 샘플의 수를 제한하면서 결정 경계 도로의 폭이 최대가 되도록 하는 기울기 찾기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**목적함수**\n",
    "\n",
    "* 결정 경계면의 기울기 $\\| \\mathbf w \\|$를 최소화하는 것과 아래 식을 최소화하는 것이 동일한 의미임.\n",
    "    따라서 아래 식을 목적함수로 지정함.\n",
    "\n",
    "$$\\frac 1 2 \\| \\mathbf w \\|^2 = \\frac 1 2 \\mathbf w^T \\mathbf w$$\n",
    "    \n",
    "\n",
    "* 이유: 함수의 미분가능성 때문에 수학적으로 다루기가 보다 쉬움. $1/2$ 또한 계산의 편의를 위해 추가됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**하드 마진 선형 SVM 분류기의 목적 함수**\n",
    "\n",
    "* 목적함수를 최소화하는 파라미터 벡터 $\\mathbf{w}$를 구하기 위해 다음 __최적화 문제__를 해결해야 함.\n",
    "\n",
    "$$\\frac 1 2 \\mathbf w^T \\mathbf w$$\n",
    "    \n",
    "\n",
    "$$\n",
    "\\text{(조건)}\\quad t^{(i)} (\\mathbf w^T \\mathbf x^{(i)} + b) \\ge 1\n",
    "$$\n",
    "\n",
    "* 즉, 모든 샘플 $\\mathbf{x}^{(i)}$에 대해 만족시켜야 하는 조건이 추가되었음. \n",
    "    $t^{(i)}$는 $i$ 번째 샘플의 클래스(양성/음성)를 가리킴.\n",
    "\n",
    "\n",
    "$$\n",
    "t^{(i)} = \n",
    "\\begin{cases}\n",
    "-1 & \\text{$x^{(i)}$가 음성인 경우} \\\\\n",
    "1 & \\text{$x^{(i)}$가 양성인 경우} \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**조건식의 의미**\n",
    "\n",
    "$$\n",
    "\\text{(조건)}\\quad t^{(i)} (\\mathbf w^T \\mathbf x^{(i)} + b) \\ge 1\n",
    "$$\n",
    "\n",
    "위 조건식의 의미는 다음과 같다.\n",
    "\n",
    "* $\\mathbf x^{(i)}$가 양성인 경우\n",
    "    - $t^{(i)} = 1$\n",
    "    - 따라서 $\\mathbf w^T \\mathbf x^{(i)} + b \\ge 1$, 즉 양성으로 예측해야 함.\n",
    "\n",
    "* $\\mathbf x^{(i)}$가 음성인 경우\n",
    "    - $t^{(i)} = -1$\n",
    "    - 따라서 $\\mathbf w^T \\mathbf x^{(i)} + b \\le -1$, 즉 음성으로 예측해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**소프트 마진 선형 SVM 분류기의 목적 함수**\n",
    "\n",
    "* 목적함수와 조건이 다음과 같음.\n",
    "\n",
    "$$\\frac 1 2 \\mathbf w^T \\mathbf w + C \\sum_{i=0}^{m-1} \\zeta^{(i)}$$    \n",
    "\n",
    "$$\\text{(조건)}\\quad t^{(i)} (\\mathbf w^T \\mathbf x^{(i)} + b) \\ge 1 - \\zeta^{(i)}$$\n",
    "    \n",
    "\n",
    "* $\\zeta^{(i)}\\ge 0$: __슬랙 변수__. $i$ 번째 샘플에 대한 마진 오류 허용 정도 지정.\n",
    "    ($\\zeta$는 그리스어 알파벳이며 '체타(zeta)'라고 발음함.)\n",
    "\n",
    "* $C$: 아래 두 목표 사이의 트레이드오프를 조절하는 하이퍼파라미터\n",
    "    * 목표 1: 결정 경계 도로의 폭을 가능하면 크게 하기 위해 $\\|\\mathbf w\\|$ 값을 가능하면 작게 만들기.\n",
    "    * 목표 2: 마진 오류 수를 제한하기, 즉 슬랙 변수의 값을 작게 유지하기.\n",
    "\n",
    "- __참고:__ 결정 경계 도로의 폭, 즉 마진 폭은 결정 경계면($\\hat y = \\mathbf{w}^T \\mathbf{x} + b$)의 기울기 $\\|\\mathbf w\\|$ 에 의해 결정됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**$\\zeta$의 역할**\n",
    "\n",
    "- $\\zeta^{(i)} > 0$이면 해당 샘플 $\\mathbf{x}^{(i)}$에 대해 다음이 성립하여 마진 오류가 될 수 있음.\n",
    "    \n",
    "    $$1 - \\zeta^{(i)} \\le t^{(i)} (\\mathbf w^T \\mathbf x^{(i)} + b) < 1$$\n",
    "\n",
    "- 이유: 결정 경계면(하이퍼플레인) 상에서 보면 결정 함숫값이 $1$보다 작은 샘플이기에\n",
    "    실제 데이터셋의 공간에서는 결정 경계 도로 안에 위치하게 됨.\n",
    "    (결정 경계 도로의 양 경계는 결정 함숫값이 $1$인 샘플들로 이루어졌음.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`C`와 마진 폭의 관계**\n",
    "\n",
    "$$\\frac 1 2 \\mathbf w^T \\mathbf w + C \\sum_{i=0}^{m-1} \\zeta^{(i)}$$    \n",
    "\n",
    "$$\\text{(조건)}\\quad t^{(i)} (\\mathbf w^T \\mathbf x^{(i)} + b) \\ge 1 - \\zeta^{(i)}$$\n",
    "    \n",
    "\n",
    "- 가정: 보다 간단한 설명을 위해 편향 $b$는 $0$이거나 무시될 정도로 작다고 가정. (표준화 전처리를 사용하면 됨.)\n",
    "\n",
    "- $C$가 매우 큰 경우\n",
    "    - $\\zeta$는 $0$에 매우 가까울 정도로 아주 작아짐.\n",
    "    - 예를 들어 양성 샘플 $\\mathbf{x}^{(i)}$에 대해, 즉 $t^{(i)} = 1$, \n",
    "        $\\mathbf{w}^T \\mathbf{x}^{(i)}$ 가 $1$보다 크거나 아니면 $1$보다 아주 조금만 작아야 함.\n",
    "        즉, 결정 경계면의 기울기 $\\|w\\|$가 어느 정도 커야 함.\n",
    "    - 결정 경계의 도로폭이 좁아짐.\n",
    "\n",
    "- $C$가 매우 작은 경우\n",
    "    - $\\zeta$가 어느 정도 커도 됨.\n",
    "    - $\\mathbf{w}^T \\mathbf{x}^{(i)}$ 가 1보다 많이 작아도 됨. 즉, $\\|w\\|$ 가 작아도 됨.\n",
    "    - 결정 경계의 도로폭이 넓어짐.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 커널 SVM 작동 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**쌍대 문제**\n",
    "\n",
    "* 쌍대 문제(dual problem): 주어진 문제의 답과 동일한 답을 갖는 문제\n",
    "\n",
    "* 선형 SVM 목적 함수의 쌍대 문제: 아래 식을 최소화하는 $\\alpha$ 찾기(단, $\\alpha^{(i)} > 0$).\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)}\\alpha^{(j)} t^{(i)} t^{(j)} {\\mathbf{x}^{(i)}}^T\\mathbf{x}^{(j)} - \\sum_{j=1}^{m} \\alpha^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**쌍대 문제 활용 예제: 다항 커널**\n",
    "\n",
    "* 원래 $d$차 다항식 함수 $\\phi()$를 적용한 후에 쌍대 목적 함수의 최적화 문제를 해결해야 함.\n",
    "    즉, 아래 문제를 최소화하는 $\\alpha$를 찾는 게 쌍대문제임.\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)}\\alpha^{(j)} t^{(i)} t^{(j)} \\phi(\\mathbf{x}^{(i)})^T \\phi(\\mathbf{x}^{(j)}) - \\sum_{j=1}^{m} \\alpha^{(i)}\n",
    "$$\n",
    "\n",
    "* 하지만 다음이 성립함.\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf a)^T \\phi(\\mathbf b) = ({\\mathbf a}^T \\mathbf b)^d\n",
    "$$\n",
    "\n",
    "* 따라서 다항식 함수 $\\phi$를 적용할 필요 없이, 즉 다항 특성을 전혀 추가할 필요 없이\n",
    "    아래 함수에 대한 최적화 문제를 해결하면 다항 특성을 추가한 효과를 얻게 됨.\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)}\\alpha^{(j)} t^{(i)} t^{(j)} \\left({\\mathbf{x}^{(i)}}^T\\mathbf{x}^{(j)}\\right)^d - \\sum_{j=1}^{m} \\alpha^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**예제: 지원되는 커널**\n",
    "\n",
    "* 다항식: \n",
    "    \n",
    "\n",
    "$$K(\\mathbf a, \\mathbf b) = \\big( \\gamma \\mathbf a^T  \\mathbf b + r \\big)^d$$\n",
    "\n",
    "* 가우시안 RBF:\n",
    "\n",
    "$$K(\\mathbf a, \\mathbf b) = \\exp \\big( \\!-\\! \\gamma \\| \\mathbf a -  \\mathbf b \\|^2 \\big )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 온라인 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 경사하강법을 이용하여 선형 SVM 분류기를 직접 구현할 수 있음.\n",
    "\n",
    "* 비용함수는 아래와 같음.\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\dfrac{1}{2} \\mathbf{w}^T \\mathbf{w} \\,+\\, C {\\displaystyle \\sum_{i=1}^{m}\\max\\left(0, 1 - t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)}\n",
    "$$\n",
    "\n",
    "* 자세한 내용은 주피터 노트북의 부록 B 참조: [[html]](https://codingalzi.github.io/handson-ml2/notebooks/handson-ml2-05.html), [[구글 코랩]](https://colab.research.google.com/github/codingalzi/handson-ml2/blob/master/notebooks/handson-ml2-05.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
