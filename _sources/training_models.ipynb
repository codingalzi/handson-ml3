{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e445e359",
   "metadata": {},
   "source": [
    "(ch:trainingModels)=\n",
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aff089",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**감사의 글**\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6931e55",
   "metadata": {},
   "source": [
    "**소스코드**\n",
    "\n",
    "본문 내용의 일부를 파이썬으로 구현한 내용은 \n",
    "[(구글코랩) 모델 훈련](https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_training_models.ipynb)에서 \n",
    "확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e66dac",
   "metadata": {},
   "source": [
    "**목표**\n",
    "\n",
    "모델 훈련의 기본 작동 과정과 원리를 살펴본다. \n",
    "이를 통해 다음을 얻을 수 있다.\n",
    "\n",
    "- 적정 모델 선택\n",
    "- 적정 훈련 알고리즘 선택\n",
    "- 적정 하이퍼파라미터 선택\n",
    "- 디버깅과 오차 분석에 도움\n",
    "- 신경망 구현 및 훈련 과정 이해에 도움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5240e0",
   "metadata": {
    "colab_type": "text",
    "id": "_6ptLsZo9knQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**주요 내용**\n",
    "\n",
    "* 수학적으로 선형 회귀 모델 구하기\n",
    "* 경사하강법으로 선형 회귀 모델 구하기\n",
    "* 경사하강법 종류\n",
    "    * 배치 경사하강법\n",
    "    * 미니배치 경사하강법\n",
    "    * 확률적 경사하강법(SGD)\n",
    "* 다항 회귀: 비선형 모델 훈련법    \n",
    "* 학습 곡선: 과소, 과대 적합 감지\n",
    "* 규제 선형 모델\n",
    "    * 과대적합 위험 감소시키기\n",
    "* 로지스틱 회귀와 소프트맥스 회귀\n",
    "    * 회귀 모델을 분류기로 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c3536",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0896a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 모델 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1571c4",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 한 개의 특성 $x_1$을 사용하는 $i$ 번째 훈련 샘플에 대한 예측값\n",
    "\n",
    "$$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c93da",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- $n\\ge 1$ 개의 특성을 사용하는 $i$번째 훈련 샘플에 대한 예측값 \n",
    "    - 예제: 캘리포니아 주택 가격 예측 모델\n",
    "\n",
    "    $$\\hat y^{(i)} = \\theta_0 + \\theta_1\\, x_1^{(i)} + \\cdots + \\theta_{16}\\, x_{16}^{(i)}$$\n",
    "    ```\n",
    "    ```\n",
    "    * $\\hat y^{(i)}$: $i$ 번째 훈련 샘플에 대한 예측값\n",
    "    * $x_k^{(i)}$: $i$ 번째 훈련 샘플의 $k$ 번째 특성값\n",
    "    * $\\theta_k$: 편향($\\theta_0$) 및 $k$ 번째 특성에 대한 가중치 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8abce6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 예측 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ece6c",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\hat y^{(i)} &  \n",
    "= \\theta_0 + \\theta_1\\, x_1^{(i)} + \\cdots + \\theta_n\\, x_n^{(i)}\\\\\n",
    "& = \\theta^T\\, \\mathbf{x}^{(i)} \\\\\n",
    "& = h_\\theta (\\mathbf{x}^{(i)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f4639",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\mathbf{x}^{(i)} = [1, x_1^{(i)}, \\dots, x_n^{(i)}]^T$. \n",
    "    - $n$은 특성 개수\n",
    "    - 1이 추가됨에 주의할 것.\n",
    "* $\\theta = [\\theta_0, \\theta_1, \\dots, \\theta_n]^T$\n",
    "* $h_\\theta(\\cdot)$: 예측 함수, 즉 모델의 `predict()` 메서드를 가리킴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f08c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 모델의 행렬 연산 표기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd113130",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\mathbf{X}$: 전체 훈련 세트, 즉 모든 훈련 샘플을 모아놓은 행렬. \n",
    "    - $m$은 훈련 세트의 크기.\n",
    "    - $\\mathbf{X}$는 $(m, n+1)$ 모양의 행렬\n",
    "\n",
    "$$\n",
    "\\mathbf{X}= \n",
    "\\begin{bmatrix} \n",
    "(\\mathbf{x}^{(1)})^T \\\\\n",
    "\\vdots \\\\\n",
    "(x_n^{(m)})^T\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fc9fb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 넘파이 2차원 어레이 표현\n",
    "\n",
    "| 데이터 | 어레이 기호           |     어레이 모양(shape) | \n",
    "|:-------------:|:-------------:|:---------------:|\n",
    "| 레이블, 예측값 | $\\mathbf y$, $\\hat{\\mathbf y}$  | $(m, 1)$ |\n",
    "| 가중치 | $\\theta$      | $(n+1, 1)$ |\n",
    "| 훈련 세트 | $\\mathbf X$   | $(m, n+1)$     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6daec16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 비용함수: 평균 제곱 오차(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb6168",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* MSE를 활용한 선형 회귀 모델 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07143fcd",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$$\n",
    "\\mathrm{MSE}(\\theta) := \\mathrm{MSE}(\\mathbf X, h_\\theta) = \n",
    "\\frac 1 m \\sum_{i=1}^{m} \\big(\\theta^{T}\\, \\mathbf{x^{(i)}} - y^{(i)}\\big)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941759e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 목표: $\\mathrm{MSE}(\\theta)$가 최소가 되도록 하는 $\\theta$ 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d08ea",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* __참고:__ $m, \\mathbf{x}^{(i)}, y^{(i)}$은 모두 주어졌음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a527bfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 방식 1: 정규방정식 또는 특이값 분해(SVD) 활용\n",
    "    * 드물지만 수학적으로 비용함수를 최소화하는 $\\theta$ 값을 직접 계산할 수 있는 경우 활용\n",
    "    * 계산복잡도가 $O(n^2)$ 이상인 행렬 연산을 수행해야 함. \n",
    "    * 따라서 특성 수($n$)이 큰 경우 메모리 관리 및 시간복잡도 문제때문에 비효율적임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7a753",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 방식 2: 경사하강법\n",
    "    * 특성 수가 매우 크거나 훈련 샘플이 너무 많아 메모리에 한꺼번에 담을 수 없을 때 적합\n",
    "    * 일반적으로 선형 회귀 모델 훈련에 적용되는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427078b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1.1 정규 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78bfd3",
   "metadata": {},
   "source": [
    "정규 방정식을 이용하여 비용함수를 최소화 하는 $\\theta$를 아래와 같이 구할 수 있음:\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \n",
    "(\\mathbf{X}^T\\, \\mathbf{X})^{-1}\\, \\mathbf{X}^T\\, \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255589e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### SVD(특잇값 분해) 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d58f1",
   "metadata": {},
   "source": [
    "* 행렬 연산과 역행렬 계산은 계산 복잡도가 $O(n^{2.4})$ 이상이고 항상 역행렬 계산이 가능한 것도 아님.\n",
    "\n",
    "* 반면에, 특잇값 분해를 활용하여 얻어지는 무어-펜로즈(Moore-Penrose) 유사 역행렬 $\\mathbf{X}^+$ 계산이 보다 효율적임.\n",
    "    계산 복잡도는 $O(n^2)$.\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \n",
    "\\mathbf{X}^+\\, \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7415b",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.2 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17f6f7",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b02a51",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 훈련 세트를 이용한 훈련 과정 중에 가중치 등과 같은 **파라미터를 조금씩 반복적으로 조정하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b4e6e",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 조정 기준: 비용 함수의 크기 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f4457",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 관련 주요 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de23f3d",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 최적 학습 모델\n",
    "\n",
    "* 비용함수를 최소화하는 또는 효용함수를 최대화하는 파라미터를 사용하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb9ad9",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 파라미터\n",
    "\n",
    "* 예측값을 생성하는 함수로 구현되는 학습 모델에 사용되는 파라미터\n",
    "* 예제: 선형 회귀 모델에 사용되는 편향과 가중치 파라미터 \n",
    "\n",
    "$$\\theta = \\theta_0,\\theta_1, \\dots, \\theta_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebcbc2b",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 비용함수\n",
    "\n",
    "* 모델이 얼마나 나쁜지를 계산해주는 함수\n",
    "* 예제: 선형 회귀 모델의 평균 제곱 오차(MSE)\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\theta) = \n",
    "\\frac 1 m \\sum_{i=1}^{m} \\big(\\theta^{T}\\, \\mathbf{x^{(i)}} - y^{(i)}\\big)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474aa0b",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 전역 최솟값\n",
    "\n",
    "* 비용함수가 가질 수 있는 최솟값\n",
    "* 예제: 선형 회귀 모델의 평균 제곱 오차(MSE) 함수가 갖는 최솟값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c9b26",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 그레이디언트 벡터\n",
    "\n",
    "* 다변수 함수의 미분값. \n",
    "\n",
    "* (그레이디언트) 벡터는 방향과 크기에 대한 정보 제공\n",
    "\n",
    "* 그레이디언트가 가리키는 방향의 __반대 방향__으로 움직여야 가장 빠르게 전역 최솟값에 접근\n",
    "\n",
    "* 예제: 선형 회귀 MSE의 그레이디언트 벡터 $\\nabla_\\theta \\textrm{MSE}(\\theta)$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\textrm{MSE}(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial \\theta_0} \\textrm{MSE}(\\theta) \\\\\n",
    "    \\frac{\\partial}{\\partial \\theta_1} \\textrm{MSE}(\\theta) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial \\theta_n} \\textrm{MSE}(\\theta)\n",
    "\\end{bmatrix} =\n",
    "\\frac{2}{m}\\, \\mathbf{X}^T\\, (\\mathbf{X}\\, \\theta^T - \\mathbf y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbae0f",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습률\n",
    "\n",
    "* 훈련 과정에서의 비용함수 파라미터 조정 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ea3f9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "##### 예제: 선형회귀 모델 파라미터 조정 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764dbfc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\theta$를 임의의 값으로 지정한 후 훈련 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed3a15",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 아래 단계를 $\\theta$가 특정 값에 지정된 오차범위 내로 수렴할 때까지 반복\n",
    "    1. (배치 크기로) 지정된 수의 훈련 샘플을 이용하여 학습.\n",
    "    2. 학습 후 $\\mathrm{MSE}(\\theta)$ 계산.\n",
    "    3. 이전 $\\theta$에서 $\\nabla_\\theta \\textrm{MSE}(\\theta)$과 학습률 $\\eta$를 곱한 값 빼기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d634a9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-01.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff2e14",
   "metadata": {},
   "source": [
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60dcfc9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$$\\theta^{(\\text{new})} = \\theta^{(\\text{old})}\\, -\\, \\eta\\cdot \\nabla_\\theta \\textrm{MSE}(\\theta^{(\\text{old})})$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5a447",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 학습률이 너무 작은 경우: 비용 함수가 전역 최소값에 너무 느리게 수렴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9f8a9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-02.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbbca7",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 학습률이 너무 큰 경우: 비용 함수가 수렴하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687189a3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-03.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8d561",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* (선형 회귀가 아닌 경우에) 시작점에 따라 지역 최솟값에 수렴하지 못할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca875c9c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-04.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce15a97",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 선형 회귀와 학습률\n",
    "\n",
    "    * 비용함수(MSE)가 볼록 함수. 즉, 지역 최솟값을 갖지 않음\n",
    "    * 따라서 학습률이 너무 크지 않으면 언젠가는 전역 최솟값에 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179909e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 특성 스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722d84e",
   "metadata": {},
   "source": [
    "* 특성들의 스켈일을 통일시키면 보다 빠른 학습 이루어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e2ee8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-04a.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fedac5",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 하이퍼파라미터(hyperparameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb8b3f",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 학습 모델을 지정할 때 사용되는 값.\n",
    "    학습률, 배치 크기, 에포크, 허용오차, 스텝 크기 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be3ee5",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 에포크(epoch): 훈련 세트 크기만큼의 샘플을 훈련하는 단계\n",
    "    * 에포크 수: 에포크 반복 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82d87e",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 배치(batch) 크기: 파라미터를 업데이트하기 위해, 즉 그레이디언트 벡터를 계산하기 위해 사용되는 훈련 샘플 수. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d9fd9",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 허용오차(tolerance): 비용함수의 그레이디언트 벡터의 크기가 허용오차보다 작아지면 학습 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aadc23",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 스텝(step): 지정된 배치 크기의 샘플을 학습한 후에 파라미터를 조정하는 단계\n",
    "    * 스텝 크기 = (훈련 샘플 수) / (배치 크기)\n",
    "    * 예제: 훈련 세트의 크기가 1,000이고 배치 크기가 10이면, \n",
    "        하나의 에포크 기간동안 총 100번의 스텝이 실행됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9e300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce0548",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 배치 경사 하강법\n",
    "\n",
    "* 전체 훈련 샘플을 대상으로 훈련한 후에, 즉 에포크마다 그레이디언트를 계산하여 파라미터 조정\n",
    "* __주의__: 여기서 사용되는 '배치'의 의미가 '배치 크기'의 '배치'와 다른 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19200f88",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 확률적 경사 하강법\n",
    "\n",
    "* 배치 크기: 1\n",
    "* 즉, 하나의 훈련 샘플을 학습할 때마다 그레이디언트를 계산해서 파라미터 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b80ad",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 미니배치 경사 하강법\n",
    "\n",
    "* 배치 크기: 2에서 수백 사이\n",
    "* 최적 배치 크기: 경우에 따라 다름. 여러 논문이 32 이하 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08494b3a",
   "metadata": {
    "colab_type": "text",
    "id": "pEegSK8KMzhA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2.1 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef6b81f",
   "metadata": {
    "colab_type": "text",
    "id": "854ojx1fOtqk",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 에포크와 허용오차\n",
    "\n",
    "    * 에포크 수는 크게 설정한 후 허용오차를 지정하여 학습 시간 제한 필요.\n",
    "        이유는 포물선의 최솟점에 가까워질 수록 그레이디언트 벡터의 크기가 0에 수렴하기 때문임.\n",
    "\n",
    "    * 허용오차와 에포크 수는 서로 반비례의 관계임. 즉, 오차를 1/10로 줄이려면 에포크 수를 10배 늘려야함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40dd341",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 단점\n",
    "\n",
    "    * 훈련 세트가 크면 그레이디언트를 계산하는 데에 많은 시간 필요\n",
    "    * 아주 많은 데이터를 저장해야 하는 메모리 문제도 발생 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e82db",
   "metadata": {
    "colab_type": "text",
    "id": "pEegSK8KMzhA",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* __주의사항__\n",
    "\n",
    "    * 사이킷런은 배치 경사 하강법을 활용한 선형 회귀 지원하지 않음.\n",
    "        (책 176쪽, 표 4-1에서 사이킷런의 SGDRegressor가 배치 경사 하강법을 지원한다고 __잘못__ 명시됨.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58281dfd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습율과 경사 하강법의 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af39970",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-04b.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe1f0e",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2.2 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890df863",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 장점\n",
    "\n",
    "    * 매우 큰 훈련 세트를 다룰 수 있음.\n",
    "        예를 들어, 외부 메모리(out-of-core) 학습을 활용할 수 있음\n",
    "    * 학습 과정이 매우 빠르며 파라미터 조정이 불안정 할 수 있기 때문에 지역 최솟값에 상대적으로 덜 민감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c159d06",
   "metadata": {
    "colab_type": "text",
    "id": "ywTb5DJhPwJD",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 단점: 학습 과정에서 파라미터의 동요가 심해서 경우에 따라 전역 최솟값에 수렴하지 못하고 계속해서 발산할 가능성도 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9763d7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-04c.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de2b0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "처음 20 단계 동안의 SGD 학습 내용: 모델이 수렴하지 못함을 확인할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85daf6f4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-04d.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebf638",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 학습 스케줄\n",
    "\n",
    "* 요동치는 파라미터를 제어하기 위해 학습률을 학습 과정 동안 천천히 줄어들게 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16d94f",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 주의사항\n",
    "    * 학습률이 너무 빨리 줄어들면, 지역 최솟값에 갇힐 수 있음\n",
    "    * 학습률이 너무 느리게 줄어들면 전역 최솟값에 제대로 수렴하지 못하고 맴돌 수 있음\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55c49d",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 학습 스케줄(learning schedule)\n",
    "    * 훈련이 지속될 수록 학습률을 조금씩 줄이는 기법\n",
    "    * 에포크, 훈련 샘플 수, 학습되는 샘플의 인덱스에 따른 학습률 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde60cf5",
   "metadata": {
    "colab_type": "text",
    "id": "oVXwxMY-SimN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 사이킷런의 `SGDRegressor`\n",
    "\n",
    "* 경사 하강법 사용\n",
    "\n",
    "* 사용되는 하이퍼파라미터\n",
    "  * `max_iter=1000`: 에포크 수 제한\n",
    "  * `tol=1e-3`: 하나의 에포크가 지날 때마다 0.001보다 적게 손실이 줄어들 때까지 훈련.\n",
    "  * `eta0=0.1`: 학습 스케줄 함수에 사용되는 매개 변수. 일종의 학습률.\n",
    "  * `penalty=l2`: 규제 사용 여부 결정 (추후 설명)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01eae7",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2.3 미니배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f77a8",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 장점\n",
    "\n",
    "    * 배치 크기를 어느 정도 크게 하면 확률적 경사 하강법(SGD) 보다 파라미터의 움직임이 덜 불규칙적이 됨\n",
    "    * 반면에 배치 경사 하강법보다 빠르게 학습\n",
    "    * 학습 스케줄 잘 활용하면 최솟값에 수렴함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b022e67",
   "metadata": {
    "colab_type": "text",
    "id": "JuFhsxFZTzvi",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 단점\n",
    "\n",
    "    * SGD에 비해 지역 최솟값에 수렴할 위험도가 보다 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd85ef0",
   "metadata": {
    "colab_type": "text",
    "id": "6IvmA4ZvU3EJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ba645",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-05.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afc183",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 알고리즘 비교\n",
    "\n",
    "\n",
    "| 알고리즘   | 많은 샘플 수 | 외부 메모리 학습 | 많은 특성 수 | 하이퍼 파라미터 수 | 스케일 조정 | 사이킷런 지원 |\n",
    "|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n",
    "| 정규방정식  | 빠름       | 지원 안됨      |  느림        | 0          | 불필요    | 지원 없음      |\n",
    "| SVD      | 빠름       | 지원 안됨      |  느림        | 0          | 불필요     | LinearRegression     |\n",
    "| 배치 GD   | 느림       | 지원 안됨      |  빠름        | 2          | 필요      | LogisticRegression      |\n",
    "| SGD      | 빠름       | 지원          |  빠름        | >= 2       | 필요      | SGDRegressor |\n",
    "| 미니배치 GD | 빠름       | 지원         |  빠름        | >=2        | 필요      | 지원 없음      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35528b09",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.3 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f850604",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 다항 회귀(polynomial regression)란?\n",
    "    * 선형 회귀를 이용하여 비선형 데이터를 학습하는 기법\n",
    "    * 즉, 비선형 데이터를 학습하는 데 선형 모델 사용을 가능하게 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78028b",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 기본 아이디어\n",
    "    * 특성들의 조합 활용\n",
    "    * 특성 변수들의 다항식을 조합 특성으로 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04bc86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 선형 회귀 vs. 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46124512",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 선형 회귀: 1차 선형 모델\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\, x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67010cb8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-06.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f619e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 다항 회귀: 2차 다항식 모델\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\, x_1 + \\theta_2\\, x_1^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f96a02",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-07.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58b0e7",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 사이킷런의 `PolynomialFeatures` 변환기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796db76",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 주어진 특성들의 거듭제곱과 특성들 사이의 곱셈을 실행하여 특성을 추가하는 기능 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be96e9e",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* `degree=d`: 몇 차 다항식을 활용할지 지정하는 하이퍼파라미터\n",
    "\n",
    "    * 이전 예제: $d=2$으로 지정하여  $x_1^2$에 대한 특성 변수가 추가됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346eb4e",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 예제: $n=2, d=3$인 경우에 $(x_1+x_2)^2$과 $(x_1+x_2)^3$의 항목에 해당하는 7개 특성 추가\n",
    "\n",
    "$$x_1^2,\\,\\, x_1 x_2,\\,\\, x_2^2,\\,\\, x_1^3,\\,\\, x_1^2 x_2,\\,\\, x_1 x_2^2,\\,\\, x_2^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79f19b",
   "metadata": {
    "colab_type": "text",
    "id": "Mt1fZDkqcCSM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.4 학습 곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81881a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과소적합/과대적합 판정\n",
    "\n",
    "* 예제: 선형 모델, 2차 다항 회귀 모델, 300차 다항 회귀 모델 비교\n",
    "\n",
    "* 다항 회귀 모델의 차수에 따라 훈련된 모델이 훈련 세트에 과소 또는 과대 적합할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c2926",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-08.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e13b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 교차 검증 vs. 학습 곡선\n",
    "\n",
    "* 교차 검증(2장)\n",
    "    * 과소적합: 훈련 세트와 교차 검증 점수 모두 낮은 경우\n",
    "    * 과대적합: 훈련 세트에 대한 검증은 우수하지만 교차 검증 점수가 낮은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef87072",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 학습 곡선 살피기\n",
    "    * 학습 곡선: 훈련 세트와 검증 세트에 대한 모델 성능을 비교하는 그래프\n",
    "    * 학습 곡선의 모양에 따라 과소적합/과대적합 판정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeae0f1",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 과소적합 모델의 학습 곡선 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871af1a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-09.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b8baf",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 훈련 데이터(빨강)에 대한 성능\n",
    "    * 훈련 세트가 커지면서 RMSE(평균 제곱근 오차)가 커짐\n",
    "    * 훈련 세트가 어느 정도 커지면 더 이상 RMSE가 변하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc6ec5",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 검증 데이터(파랑)에 대한 성능\n",
    "    * 검증 세트에 대한 성능이 훈련 세트에 대한 성능과 거의 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40625c55",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### 과대적합 모델의 학습 곡선 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af20722",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-10.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b37544",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 훈련 데이터(빨강)에 대한 성능: 훈련 데이터에 대한 평균 제곱근 오차가 매우 낮음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828a815",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 검증 데이터(파랑)에 대한 성능: 훈련 데이터에 대한 성능과 차이가 크게 벌어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c51edd",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 과대적합 모델 개선법: 훈련 데이터 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62842c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 편향 vs 분산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701b61d",
   "metadata": {},
   "source": [
    "* 편향(bias)\n",
    "    - 실제로는 2차원 모델인데 1차원 모델을 사용하는 경우처럼 잘못된 가정으로 인해 발생.\n",
    "    - 과소적합 발생 가능성 높음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952f67b",
   "metadata": {},
   "source": [
    "* 분산(variance)\n",
    "    - 모델이 훈련 데이터에 민감하게 반응하는 정도\n",
    "    - 고차 다항 회귀 모델의 경우 분산이 높아질 수 있음.\n",
    "    - 과대적합 발생 가능성 높음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bad00f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 편향과 분산의 트레이드 오프\n",
    "    - 복잡한 모델일 수록 편향을 줄어들지만 분산을 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a647d58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 모델 일반화 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f2b89",
   "metadata": {},
   "source": [
    "* 훈련 후에 새로운 데이터 대한 예측에서 발생하는 오차를 가리키며 세 종류의 오차가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e42e4",
   "metadata": {},
   "source": [
    "- 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff2df3",
   "metadata": {},
   "source": [
    "- 분산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e56453",
   "metadata": {},
   "source": [
    "- 줄일 수 없는 오차\n",
    "    - 데이터 자체가 갖고 있는 잡음(noise) 때문에 발생.\n",
    "    - 잡음을 제거해야 오차를 줄일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586951f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.5 규제를 사용하는 선형 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb18c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 자유도와 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a3a45",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 자유도(degree of freedom): 학습 모델 결정에 영향을 주는 요소(특성)들의 수\n",
    "    * 단순 선형 회귀의 경우: 특성 수\n",
    "    * 다항 선형 회귀 경우: 차수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdadae6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 규제(regularization): 자유도 제한\n",
    "    * 단순 선형 회귀 모델에 대한 규제: 가중치 역할 제한\n",
    "    * 다항 선형 회귀 모델에 대한 규제: 차수 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32325e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 가중치를 규제하는 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddcb32",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 릿지 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021c293",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 라쏘 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3733f17",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 엘라스틱넷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ca1d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 규제 적용 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3656fed",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "규제항은 훈련 과정에만 사용된다. 테스트 과정에는 다른 기준으로 성능을 평가한다.\n",
    "\n",
    "* 훈련 과정: 비용 최소화 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5497c0",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 테스트 과정: 최종 목표에 따른 성능 평가\n",
    "    * 예제: 분류기의 경우 재현율/정밀도 기준으로 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef747bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5.1 릿지 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f7753",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + \\alpha \\, \\frac{1}{2} \\sum_{i=1}^{n}\\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab28d6",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\alpha$(알파): 규제 강도 지정. \n",
    "    $\\alpha=0$이면 규제가 전혀 없는 기본 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c60df",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\alpha$가 커질 수록 가중치의 역할이 줄어듦. \n",
    "    비용을 줄이기 위해 가중치를 작게 유지하는 방향으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c82e8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\theta_0$은 규제하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcb2c9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 주의사항: 특성 스케일링 전처리를 해야 성능이 좋아짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa374f48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5.2 라쏘 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e34e9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + \\alpha \\, \\sum_{i=1}^{n}\\mid\\theta_i\\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd95ddb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\alpha$(알파): 규제 강도 지정.\n",
    "    $\\alpha=0$이면 규제가 전혀 없는 기본 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d0434",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\theta_i$: 덜 중요한 특성을 무시하기 위해 $\\mid\\theta_i\\mid$가 0에 수렴하도록 학습 유도."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead4d71",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* $\\theta_0$은 규제하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d72d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 라쏘 회귀 대 릿지 회귀 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c9803",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/lasso_vs_ridge_plot.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a5587",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5.3 엘라스틱넷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df25b67",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + r\\, \\alpha \\, \\sum_{i=1}^{n}\\mid\\theta_i\\mid + \\,\\frac{1-r}{2}\\, \\alpha\\, \\sum_{i=1}^{n}\\theta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b4129",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 릿지 회귀와 라쏘 회귀를 절충한 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d260a583",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 혼합 비율 $r$을 이용하여 릿지 규제와 라쏘 규제를 적절하게 조절"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456c12b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 규제 사용 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ba6db",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 대부분의 경우 약간이라도 규제 사용 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10027549",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 릿지 규제가 기본"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d79c0",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 유용한 속성이 많지 않다고 판단되는 경우 \n",
    "    * 라쏘 규제나 엘라스틱넷 활용 추천\n",
    "    * 불필요한 속성의 가중치를 0으로 만들기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0622877",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 특성 수가 훈련 샘플 수보다 크거나 특성 몇 개가 강하게 연관되어 있는 경우\n",
    "    * 라쏘 규제는 적절치 않음.\n",
    "    * 엘라스틱넷 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b176901",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5.4 조기 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15dbcbf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 모델의 훈련 세트에 대한 과대 적합 방지를 위해 훈련을 적절한 시기에 중단시키기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae8eeb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 조기 종료: 검증 데이터에 대한 손실이 줄어 들다가 다시 커지는 순간 훈련 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2765a7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-11.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5f945",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 확률적 경사 하강법 등의 경우 손실 곡선의 진동 발생. \n",
    "    검증 손실이 한동안 최솟값보다 높게 유지될 때 훈련 멈춤. 최소 검증 손실 모델 확인."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980e759",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.6 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd1c9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "회귀 모델을 분류 모델로 활용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513bf38",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 이진 분류: 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c4f1d",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 다중 클래스 분류: 소프트맥스 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f6b0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.6.1 확률 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdbbc5a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 시그모이드 함수\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55a70a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-12.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10eded",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 로지스틱 회귀 모델에서 샘플 $\\mathbf x$가 양성 클래스에 속할 확률\n",
    "\n",
    "$$\\hat p = h_\\theta (\\mathbf x)\n",
    "= \\sigma(\\theta_0 + \\theta_1\\, x_1 + \\cdots + \\theta_n\\, x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88c12d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 예측값\n",
    "\n",
    "$$\n",
    "\\hat y = \n",
    "\\begin{cases}\n",
    "0 & \\text{if}\\,\\, \\hat p < 0.5 \\\\\n",
    "1 & \\text{if}\\,\\, \\hat p \\ge 0.5\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e289ae",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 양성 클래스인 경우: \n",
    "\n",
    "$$\\theta_0 + \\theta_1\\, x_1 + \\cdots + \\theta_n\\, x_n \\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f2d34",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 음성 클래스인 경우: \n",
    "\n",
    "$$\\theta_0 + \\theta_1\\, x_1 + \\cdots + \\theta_n\\, x_n < 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c15f8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.6.2 훈련과 비용함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7f6b3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수: 로그 손실(log loss) 함수 사용\n",
    "\n",
    "$$\n",
    "J(\\theta) = \n",
    "- \\frac{1}{m}\\, \\sum_{i=1}^{m}\\, [y^{(i)}\\, \\log(\\,\\hat p^{(i)}\\,) + (1-y^{(i)})\\, \\log(\\,1 - \\hat p^{(i)}\\,)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0799fd97",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 모델 훈련: 위 비용함수에 대해 경사 하강법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c5d48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 로그 손실 함수 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b05699",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 틀린 예측을 하면 손실값이 많이 커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1c6c9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$$\n",
    "- [y\\, \\log(\\,\\hat p\\,) + (1-y)\\, \\log(\\,1 - \\hat p\\,)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275b854",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-12-10a.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8abe5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 로그 손실 함수의 편도 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd5b0e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{\\partial}{\\partial \\theta_j} \\text{J}(\\boldsymbol{\\theta}) = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\left(\\mathbf{\\sigma(\\boldsymbol{\\theta}}^T \\mathbf{x}^{(i)}) - y^{(i)}\\right)\\, x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd511a",
   "metadata": {},
   "source": [
    "* 편도 함수가 선형 회귀의 경우와 매우 비슷한 것에 대한 확률론적 근거가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f08b02",
   "metadata": {},
   "source": [
    "* __참고:__ [앤드류 응(Andrew Ng) 교수의 Stanford CS229](https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f060ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.6.3 결정 경계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f080b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 예제: 붓꽃 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0730a42",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 꽃받침(sepal)과 꽃입(petal)과 관련된 4개의 특성 사용\n",
    "    * 꽃받침 길이\n",
    "    * 꽃받침 너비\n",
    "    * 꽃잎 길이\n",
    "    * 꽃잎 너비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9fb03",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 타깃: 세 개의 품종\n",
    "    * 0: Iris-Setosa(세토사)\n",
    "    * 1: Iris-Versicolor(버시컬러)\n",
    "    * 2: Iris-Virginica(버지니카)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1bc06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 꽃잎의 너비를 기준으로 Iris-Virginica 여부 판정하기\n",
    "\n",
    "* 결정경계: 약 1.6cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32161b40",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-14.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b398ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 꽃잎의 너비와 길이를 기준으로 Iris-Virginica 여부 판정하기\n",
    "\n",
    "* 결정경계: 검정 점선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002916e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-15.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cde1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 로지스틱 회귀 규제하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f134d3c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 하이퍼파라미터 `penalty`와 `C` 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c76a8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* `penalty`\n",
    "    * `l1`, `l2`, `elasticnet` 세 개중에 하나 사용.\n",
    "    * 기본은 `l2`, 즉, $\\ell_2$ 규제를 사용하는 릿지 규제.\n",
    "    * `elasticnet`을 선택한 경우 `l1_ration` 옵션 값을 함께 지정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0dda02",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* `C`\n",
    "    * 릿지 또는 라쏘 규제 정도를 지정하는 $\\alpha$의 역수에 해당. \n",
    "    * 따라서 0에 가까울 수록 강한 규제 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13540217",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.6.4 소프트맥스(softmax) 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dc3f5",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 로지스틱 회귀 모델을 일반화하여 다중 클래스 분류를 지원하도록 한 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef533f4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* **다항 로지스틱 회귀** 라고도 불림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d155fc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 주의사항: 소프트맥스 회귀는 다중 출력 분류 지원 못함. \n",
    "    예를 들어, 하나의 사진에서 여러 사람의 얼굴 인식 불가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9094eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 소프트맥스 회귀 학습 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f47d69",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 샘플 $\\mathbf x$가 주어졌을 때 각각의 분류 클래스 $k$ 에 대한 점수 $s_k(\\mathbf x)$ 계산.\n",
    "    즉, `k*(n+1)` 개의 파라미터를 학습시켜야 함.\n",
    "\n",
    "$$\n",
    "s_k(\\mathbf x) = \\theta_0^{(k)} + \\theta_1^{(k)}\\, x_1 + \\cdots + \\theta_n^{(k)}\\, x_n\n",
    "$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79fc91",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* __소프트맥스 함수__를 이용하여 각 클래스 $k$에 속할 확률 $\\hat p_k$ 계산\n",
    "\n",
    "$$\n",
    "\\hat p_k = \n",
    "\\frac{\\exp(s_k(\\mathbf x))}{\\sum_{j=1}^{K}\\exp(s_j(\\mathbf x))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c4abb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 추정 확률이 가장 높은 클래스 선택\n",
    "\n",
    "$$\n",
    "\\hat y = \n",
    "\\mathrm{argmax}_k s_k(\\mathbf x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de7152",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 소프트맥스 회귀 비용함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eead2e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 각 분류 클래스 $k$에 대한 적절한 가중치 벡터 $\\theta_k$를 학습해 나가야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10760e41",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수: 크로스 엔트로피 비용 함수 사용\n",
    "\n",
    "$$\n",
    "J(\\Theta) = \n",
    "- \\frac{1}{m}\\, \\sum_{i=1}^{m}\\sum_{k=1}^{K} y^{(i)}_k\\, \\log(\\hat{p}_k^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39da01",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 위 비용함수에 대해 경사 하강법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afba7e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $K=2$이면 로지스틱 회귀의 로그 손실 함수와 정확하게 일치."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c6cc0",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 주어진 샘플의 타깃 클래스를 제대로 예측할 경우 높은 확률값 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662246d",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 크로스 엔트로피 개념은 정보 이론에서 유래함. 자세한 설명은 생략."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ecb14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 다중 클래스 분류 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c413824",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 사이킷런의 `LogisticRegression` 예측기 활용\n",
    "    * `multi_class=multinomial`로 지정\n",
    "    * `solver=lbfgs`: 다중 클래스 분류 사용할 때 반드시 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73faf83",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 붓꽃 꽃잎의 너비와 길이를 기준으로 품종 분류\n",
    "    * 결정경계: 배경색으로 구분\n",
    "    * 곡선: Iris-Versicolor 클래스에 속할 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc91c8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/ch04/homl04-16.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6afee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
