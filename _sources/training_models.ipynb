{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e445e359",
   "metadata": {},
   "source": [
    "(ch:trainingModels)=\n",
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aff089",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**감사의 글**\n",
    "\n",
    "자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6931e55",
   "metadata": {},
   "source": [
    "**소스코드**\n",
    "\n",
    "본문 내용의 일부를 파이썬으로 구현한 내용은 \n",
    "[(구글코랩) 모델 훈련](https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_training_models.ipynb)에서 \n",
    "확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5240e0",
   "metadata": {
    "colab_type": "text",
    "id": "_6ptLsZo9knQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**주요 내용**\n",
    "\n",
    "* 선형 회귀 모델 구현\n",
    "    * 선형대수 활용\n",
    "    * 경사하강법 활용\n",
    "* 경사하강법 종류\n",
    "    * 배치 경사하강법\n",
    "    * 미니배치 경사하강법\n",
    "    * 확률적 경사하강법(SGD)\n",
    "* 다항 회귀: 비선형 회귀 모델\n",
    "* 학습 곡선: 과소, 과대 적합 감지\n",
    "* 모델 규제: 과대 적합 방지\n",
    "* 로지스틱 회귀와 소프트맥스 회귀: 분류 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c19e6",
   "metadata": {},
   "source": [
    "**목표**\n",
    "\n",
    "모델 훈련의 기본 작동 과정과 원리를 살펴보며,\n",
    "이를 통해 다음 사항들에 대한 이해를 넓힌다.\n",
    "\n",
    "- 적정 모델 선택\n",
    "- 적정 훈련 알고리즘 선택\n",
    "- 적정 하이퍼파라미터 선택\n",
    "- 디버깅과 오차 분석\n",
    "- 신경망 구현 및 훈련 과정 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c3536",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087c49f",
   "metadata": {},
   "source": [
    "**선형 회귀 예제: 1인당 GDP와 삶의 만족도**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ef4fc",
   "metadata": {
    "colab_type": "text",
    "id": "cu97Fkqb1JRm",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "{numref}`%s 절 <sec:model_based_learning>`에서 1인당 GDP와 삶의 만족도 사이의 \n",
    "관계를 다음 1차 함수로 표현할 수 있었다.\n",
    "\n",
    "$$(\\text{삶의만족도}) = \\theta_0 + \\theta_1\\cdot (\\text{1인당GDP})$$\n",
    "\n",
    "즉, 1인당 GDP가 주어지면 위 함수를 이용하여 삶의 만족도를 예측하였다.\n",
    "주어진 1인당 GDP를 **입력 특성**<font size=\"2\">input feature</font> $x$, \n",
    "예측된 삶의 만족도를 **예측값** $\\hat y$ 라 하면 다음 식으로 변환된다.\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\cdot x_1$$\n",
    "\n",
    "절편 $\\theta_0$ 와 기울기 $\\theta_1$ 은 (선형) 모델의 **파라미터**<font size=\"2\">weight parameter</font>이다.\n",
    "머신러닝에서는 절편은 **편향**<font size=\"2\">bias</font>, \n",
    "기울기는 **가중치**<font size=\"2\">weight</font> 라 부른다.\n",
    "\n",
    "따라서 1인당 GDP와 삶의 만족도 사이의 선형 관계를 모델로 구현하려면\n",
    "적절한 하나의 편향과 하나의 가중치, 즉 총 2개의 파라미터를 결정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968f815",
   "metadata": {},
   "source": [
    "**선형 회귀 예제: 캘리포니아 주택 가격 예측**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a9a50",
   "metadata": {},
   "source": [
    "반면에 {numref}`%s 장 <ch:end2end>`의 캘리포니아 주택 가격 예측 선형 회귀 모델은\n",
    "24개의 입력 특성을 사용하는 다음 함수를 이용한다.\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\cdot x_1 + \\cdots + \\theta_{24}\\cdot x_{24}$$\n",
    "\n",
    "* $\\hat y$: 예측값\n",
    "* $x_i$: 구역의 $i$ 번째 특성값(위도, 경도, 중간소득, 가구당 인원 등등등)\n",
    "* $\\theta_0$: 편향\n",
    "* $\\theta_i$: $i$ 번째 특성에 대한 (가중치) 파라미터, 단 $i > 0$.\n",
    "\n",
    "따라서 캘리포니아의 구역별 중간 주택 가격을 예측하는 선형 회귀 모델을 구하려면 \n",
    "적절한 하나의 편향과 24개의 가중치,\n",
    "즉 총 25개의 파라미터를 결정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea1284",
   "metadata": {},
   "source": [
    "**선형 회귀 함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69d176",
   "metadata": {},
   "source": [
    "이를 일반화하면 다음과 같다.\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\cdot x_1 + \\cdots + \\theta_{n}\\cdot x_{n}$$\n",
    "\n",
    "* $\\hat y$: 예측값\n",
    "* $n$: 특성 수\n",
    "* $x_i$: 구역의 $i$ 번째 특성값\n",
    "* $\\theta_0$: 편향\n",
    "* $\\theta_j$: $j$ 번째 특성에 대한 (가중치) 파라미터(단, $1 \\le j \\le n$)\n",
    "\n",
    "일반적으로 선형 회귀 모델을 구현하려면\n",
    "한 개의 편향과 $n$ 개의 가중치, 즉 총 $(1+n)$ 개의 파라미터를 결정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8abce6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**벡터 표기법**\n",
    "\n",
    "예측값을 벡터의 **내적**<font size=\"2\">inner product</font>으로 표현할 수 있다.\n",
    "\n",
    "$$\n",
    "\\hat y\n",
    "= h_\\theta (\\mathbf{x})\n",
    "= \\mathbf{\\theta} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "* $h_\\theta(\\cdot)$: 예측 함수, 즉 모델의 `predict()` 메서드.\n",
    "* $\\mathbf{x} = (1, x_1, \\dots, x_n)$\n",
    "* $\\mathbf{\\theta} = (\\theta_0, \\theta_1, \\dots, \\theta_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf8f82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2D 어레이 표기법**\n",
    "\n",
    "머신러닝에서는 훈련 샘플을 나타내는 입력 벡터와 파라미터 벡터를 일반적으로 아래 모양의 행렬로 나타낸다.\n",
    "\n",
    "$$\n",
    "\\mathbf{x}=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "\\mathbf{\\theta}=\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "따라서 예측값은 다음과 같이 행렬 연산으로 표기된다.\n",
    "단, $A^T$ 는 행렬 $A$의 전치행렬을 가리킨다.\n",
    "\n",
    "$$\n",
    "\\hat y\n",
    "= h_\\theta (\\mathbf{x})\n",
    "= \\mathbf{\\theta}^{T} \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f08c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**선형 회귀 모델의 행렬 연산 표기법**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd113130",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$\\mathbf{X}$가 전체 입력 데이터셋, 즉 전체 훈련셋을 가리키는 (m, 1+n) 모양의 2D 어레이, 즉 행렬이라 하자.\n",
    "- $m$: 훈련셋의 크기.\n",
    "- $n$: 특성 수\n",
    "\n",
    "그러면 $\\mathbf{X}$ 는 다음과 같이 표현된다.\n",
    "단, $\\mathbf{x}_j^{(i)}$ 는 $i$-번째 입력 샘플의 $j$-번째 특성값을 가리킨다.\n",
    "\n",
    "$$\n",
    "\\mathbf{X}= \n",
    "\\begin{bmatrix} \n",
    "[1, \\mathbf{x}_1^{(1)}, \\dots, \\mathbf{x}_n^{(1)}] \\\\\n",
    "\\vdots \\\\\n",
    "[1, \\mathbf{x}_1^{(m)}, \\dots, \\mathbf{x}_n^{(m)}] \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9e467",
   "metadata": {},
   "source": [
    "결론적으로 모든 입력값에 대한 예측값을 하나의 행렬식으로 표현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a4dd4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\hat y_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat y_m\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix} \n",
    "[1, \\mathbf{x}_1^{(1)}, \\dots, \\mathbf{x}_n^{(1)}] \\\\\n",
    "\\vdots \\\\\n",
    "[1, \\mathbf{x}_1^{(m)}, \\dots, \\mathbf{x}_n^{(m)}] \\\\\n",
    "\\end{bmatrix}\n",
    "\\,\\, \n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd89cb4",
   "metadata": {},
   "source": [
    "간략하게 줄이면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799eecb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mathbf y} = \\mathbf{X}\\, \\mathbf{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1185648",
   "metadata": {},
   "source": [
    "위 식에 사용된 기호들의 의미와 어레이 모양은 다음과 같다.\n",
    "\n",
    "| 데이터 | 어레이 기호           |     어레이 모양(shape) | \n",
    "|:-------------:|:-------------:|:---------------:|\n",
    "| 예측값 | $\\hat{\\mathbf y}$  | $(m, 1)$ |\n",
    "| 훈련셋 | $\\mathbf X$   | $(m, 1+n)$     |\n",
    "| 파라미터 | $\\mathbf{\\theta}$      | $(1+n, 1)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6daec16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**비용함수: 평균 제곱 오차(MSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb6168",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "회귀 모델은 훈련 중에 **평균 제곱 오차**<font size=\"2\">mean squared error</font>(MSE)를 이용하여\n",
    "성능을 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07143fcd",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$$\n",
    "\\mathrm{MSE}(\\mathbf{\\theta}) := \\mathrm{MSE}(\\mathbf X, h_{\\mathbf{\\theta}}) = \n",
    "\\frac 1 m \\sum_{i=1}^{m} \\big(\\mathbf{\\theta}^{T}\\, \\mathbf{x}^{(i)} - y^{(i)}\\big)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941759e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "최종 목표는 훈련셋이 주어졌을 때 $\\mathrm{MSE}(\\mathbf{\\theta})$가 최소가 되도록 하는 \n",
    "$\\mathbf{\\theta}$를 찾는 것이다.\n",
    "\n",
    "* 방식 1: 정규방정식 또는 특이값 분해(SVD) 활용\n",
    "    * 드물지만 수학적으로 비용함수를 최소화하는 $\\mathbf{\\theta}$ 값을 직접 계산할 수 있는 경우 활용\n",
    "    * 계산복잡도가 $O(n^2)$ 이상인 행렬 연산을 수행해야 함. \n",
    "    * 따라서 특성 수($n$)이 큰 경우 메모리 관리 및 시간복잡도 문제때문에 비효율적임.\n",
    "\n",
    "* 방식 2: 경사하강법\n",
    "    * 특성 수가 매우 크거나 훈련 샘플이 너무 많아 메모리에 한꺼번에 담을 수 없을 때 적합\n",
    "    * 일반적으로 선형 회귀 모델 훈련에 적용되는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427078b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**정규 방정식**\n",
    "\n",
    "비용함수를 최소화 하는 $\\theta$를 \n",
    "정규 방정식<font size=\"2\">normal equation</font>을 이용하여 \n",
    "아래와 같이 바로 계산할 수 있다.\n",
    "단, $\\mathbf{X}^T\\, \\mathbf{X}$ 의 역행렬이 존재해야 한다.\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}} = \n",
    "(\\mathbf{X}^T\\, \\mathbf{X})^{-1}\\, \\mathbf{X}^T\\, \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255589e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**SVD(특잇값 분해) 활용**\n",
    "\n",
    "그런데 행렬 연산과 역행렬 계산은 계산 복잡도가 $O(n^{2.4})$ 이상이며\n",
    "항상 역행렬 계산이 가능한 것도 아니다.\n",
    "반면에, **특잇값 분해**를 활용하여 얻어지는 \n",
    "**무어-펜로즈(Moore-Penrose) 유사 역행렬** $\\mathbf{X}^+$은 항상 존재하며\n",
    "계산 복잡도가 $O(n^2)$ 로 보다 빠른 계산을 지원한다.\n",
    "또한 다음이 성립한다.\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}} = \n",
    "\\mathbf{X}^+\\, \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7415b",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b02a51",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "훈련 세트를 이용한 훈련 과정 중에 가중치 파라미터를 조금씩 반복적으로 조정한다. \n",
    "이때 비용 함수의 크기를 줄이는 방향으로 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e27ad",
   "metadata": {},
   "source": [
    "**경사 하강법**<font size=\"2\">gradient descent</font>(GD) 이해를 위해 다음 개념들을 충분히 이해하고 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de23f3d",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**최적 학습 모델**\n",
    "\n",
    "비용 함수를 최소화하는 또는 효용 함수를 최대화하는 파라미터를 사용하는 모델이며,\n",
    "최종적으로 훈련시킬 대상이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb9ad9",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**파라미터<font size=\"2\">parameter</font>**\n",
    "\n",
    "선형 회귀 모델에 사용되는 편향과 가중치 파라미터처럼 모델 훈련중에 학습되는 파라미터를 가리킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebcbc2b",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**비용 함수<font size=\"2\">cost function</font>**\n",
    "\n",
    "평균 제곱 오차(MSE)처럼 모델이 얼마나 나쁜가를 측정하는 함수다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474aa0b",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**전역 최솟값<font size=\"2\">global minimum</font>**\n",
    "\n",
    "비용 함수의 전역 최솟값이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1001b",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**비용 함수의 그레이디언트 벡터**\n",
    "\n",
    "MSE를 비용함수로 사용하는 경우 $\\textrm{MSE}(\\mathbf{\\theta})$ 함수의 $\\mathbf{\\mathbf{\\theta}}$ 에 \n",
    "대한 그레이디언트<font size=\"2\">gradient</font> 벡터를 사용한다.\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf{\\theta} \\textrm{MSE}(\\mathbf{\\theta}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial \\mathbf{\\theta}_0} \\textrm{MSE}(\\mathbf{\\theta}) \\\\\n",
    "    \\frac{\\partial}{\\partial \\mathbf{\\theta}_1} \\textrm{MSE}(\\mathbf{\\theta}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial \\mathbf{\\theta}_n} \\textrm{MSE}(\\mathbf{\\theta})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbae0f",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**학습률($\\eta$)**\n",
    "\n",
    "훈련 과정에서의 비용함수의 파라미터($\\mathbf{\\theta}$)를 조정할 때 사용하는 조정 비율이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a0186",
   "metadata": {},
   "source": [
    "**에포크<font size=\"2\">epoch</font>**\n",
    "\n",
    "훈련셋에 포함된 모든 데이터를 대상으로 예측값을 계산하는 과정을 가리킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125d4b5",
   "metadata": {},
   "source": [
    "**허용오차<font size=\"2\">tolerance</font>**\n",
    "\n",
    "비용함수의 값이 허용오차보다 작아지면 훈련을 종료시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ed7a9",
   "metadata": {},
   "source": [
    "**배치 크기<font size=\"2\">batch size</font>**\n",
    "\n",
    "파라미터를 업데이트하기 위해, 즉 그레이디언트 벡터를 계산하기 위해 사용되는 훈련 데이터의 개수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f69db9",
   "metadata": {},
   "source": [
    "**하이퍼파라미터<font size=\"2\">hyperparameter</font>**\n",
    "\n",
    "학습률, 에포크, 허용오차, 배치 크기 처럼 모델을 지정할 때 사용되는 값을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413c608",
   "metadata": {},
   "source": [
    "### 선형 회귀 모델 파라미터 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ea3f9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "선형회귀 모델 파라미터를 조정하는 과정을 이용하여 경사 하강법의 기본 아이디어를 설명한다.\n",
    "\n",
    "먼저 $\\mathrm{MSE}(\\mathbf{\\theta})$ 는 $\\mathbf{\\theta}$ 에 대한 2차 함수임에 주의한다.\n",
    "여기서는 $\\mathbf{\\theta}$ 가 하나의 파라미터로 구성되었다고 가정한다.\n",
    "따라서 $\\mathrm{MSE}(\\mathbf{\\theta})$의 그래프는 포물선이 된다.\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE}(\\mathbf{\\theta}) =\n",
    "\\frac 1 m \\sum_{i=1}^{m} \\big(\\mathbf{\\theta}^{T}\\, \\mathbf{x}^{(i)} - y^{(i)}\\big)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764dbfc",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "경사 하강법은 다음 과정으로 이루어진다. \n",
    "\n",
    "1. $\\mathbf{\\theta}$를 임의의 값으로 지정한 후 훈련을 시작한다.\n",
    "\n",
    "1. 아래 단계를 $\\textrm{MSE}(\\theta)$ 가 허용오차보다 적게 작아지는 단계까지 반복한다.\n",
    "    * 지정된 수의 훈련 샘플을 이용한 학습.\n",
    "    * $\\mathrm{MSE}(\\mathbf{\\theta})$ 계산.\n",
    "    * 이전 $\\mathbf{\\theta}$에서 $\\nabla_\\mathbf{\\theta} \\textrm{MSE}(\\mathbf{\\theta})$ 와\n",
    "        학습률 $\\eta$를 곱한 값 빼기.<br><br>\n",
    "\n",
    "        $$\\theta^{(\\text{new})} = \\theta^{(\\text{old})}\\, -\\, \\eta\\cdot \\nabla_\\theta \\textrm{MSE}(\\theta^{(\\text{old})})$$    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db61b1",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "위 수식은 산에서 가장 경사가 급한 길을 따를 때 가장 빠르게 하산하는 원리와 동일하다.\n",
    "이유는 해당 지점에서 그레이디언트 벡터를 계산하면 정상으로 가는 가장 빠른 길을 안내할 것이기에\n",
    "그 반대방향으로 움직여야 하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6414eca",
   "metadata": {
    "colab_type": "text",
    "id": "3BbE04h8-tOu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{admonition} 벡터의 방향과 크기\n",
    ":class: info\n",
    "\n",
    "모든 벡터는 방향과 크기를 갖는다. \n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/vector01.png\" width=\"200\"/></div>\n",
    "\n",
    "그레이디언트 벡터 또한 방향과 크기에 대한 정보를 제공하며, \n",
    "그레이디언트가 가리키는 방향의 __반대 방향__으로 움직이면 빠르게 전역 최솟값에 접근한다.\n",
    "\n",
    "이는 아래 그림이 표현하듯이 산에서 가장 경사가 급한 길을 따를 때 가장 빠르게 하산하는 원리와 동일하다.\n",
    "이유는 해당 지점에서 그레이디언트 벡터를 계산하면 정상으로 가는 가장 빠른 길을 안내할 것이기에\n",
    "그 반대방향으로 움직여야 하기 때문이다.\n",
    "\n",
    "아래 그림은 경사 하강법을 담당하는 여러 알고리즘을 비교해서 보여준다.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"padding:1px\">\n",
    "            <figure>\n",
    "                <img src=\"https://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif\" style=\"width:90%\" title=\"SGD without momentum\">\n",
    "                <figcaption>SGD optimization on loss surface contours</figcaption>\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td style=\"padding:1px\">\n",
    "            <figure>\n",
    "                <img src=\"https://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif\" style=\"width:90%\" title=\"SGD without momentum\">\n",
    "                <figcaption>SGD optimization on saddle point</figcaption>\n",
    "            </figure>\n",
    "        </td>        \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**그림 출처:** [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202236e8",
   "metadata": {},
   "source": [
    "**학습률의 중요성**\n",
    "\n",
    "선형 회귀 모델은 적절할 학습률로 훈련될 경우 빠른 시간에 비용 함수의 최솟값에 도달한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d634a9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-01.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5a447",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "반면에 학습률이 너무 작거나 크면 비용 함수의 전역 최솟값에 수렴하지 않을 수 있다.\n",
    "\n",
    "- 학습률이 너무 작은 경우: 비용 함수가 전역 최소값에 너무 느리게 수렴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9f8a9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-02.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbbca7",
   "metadata": {
    "colab_type": "text",
    "id": "9vcJYAPEC0nA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 학습률이 너무 큰 경우: 비용 함수가 수렴하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687189a3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-03.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8d561",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "선형 회귀가 아닌 경우에는 시작점에 따라 지역 최솟값에 수렴하거나 정체될 수 있음을\n",
    "아래 그림이 잘 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca875c9c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-04.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179909e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**특성 스케일링의 중요성**\n",
    "\n",
    "특성들의 스켈일을 통일시키면 보다 빠른 학습이 이루어지는 이유를 \n",
    "아래 그림이 설명한다.\n",
    "\n",
    "* 왼편 그림: 두 특성의 스케일이 동일하게 조정된 경우 비용 함수의 최솟값으로 최단거리로 수렴한다.\n",
    "* 오른편 그림: 두 특성의 스케일이 다른 경우 비용 함수의 최솟값으로 보다 먼 거리를 지나간다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e2ee8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-04a.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9e300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 경사 하강법 종류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c691f",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "모델을 지정할 때 지정하는 배치 크기에 따라 세 종류로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5b321",
   "metadata": {
    "colab_type": "text",
    "id": "XpnLVyOXJoaU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**참고:** 지정된 배치 크기의 샘플에 대해 예측을 한 후에 경사하강법을 이용하여 파라미터를 조정하는 단계를\n",
    "스텝<font size=\"2\">step</font>이라 하며, 다음이 성립힌다.\n",
    "\n",
    "    스텝 크기 = (훈련 샘플 수) / (배치 크기)\n",
    "\n",
    "예를 들어, 훈련 세트의 크기가 1,000이고 배치 크기가 10이면, 에포크 당 100번의 스텝이 실행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08494b3a",
   "metadata": {
    "colab_type": "text",
    "id": "pEegSK8KMzhA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902408f8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "에포크마다 그레이디언트를 계산하여 파라미터를 조정한다.\n",
    "즉, 배치의 크기가 전체 훈련셋의 크기와 같고 따라서 스텝의 크기는 1이다.\n",
    "\n",
    "단점으로 훈련 세트가 크면 그레이디언트를 계산하는 데에 많은 시간과 메모리가 필요해지는 문제가 있다. \n",
    "이와 같은 이유로 인해 사이킷런은 배치 경사 하강법을 지원하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58281dfd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**학습율과 경사 하강법의 관계**\n",
    "\n",
    "학습률에 따라 파라미터(\\theta)의 수렴 여부와 속도가 달라진다.\n",
    "최적의 학습률은 그리드 탐색 등을 이용하여 찾아볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af39970",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-04b.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59933a5e",
   "metadata": {
    "colab_type": "text",
    "id": "854ojx1fOtqk",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**에포크 수와 허용오차**\n",
    "\n",
    "에포크 수는 크게 설정한 후 허용오차를 지정하여 학습 시간을 제한할 필요가 있다.\n",
    "이유는 포물선의 최솟점에 가까워질 수록 그레이디언트 벡터의 크기가 0에 수렴하기 때문이다.\n",
    "\n",
    "허용오차와 에포크 수는 서로 반비례의 관계이다. \n",
    "예를 들어, 허용오차를 1/10로 줄이려면 에포크 수를 10배 늘려야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb4c6b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 확률적 경사 하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5f87e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "배치 크기가 1이다.\n",
    "즉, 하나의 스텝에 하나의 훈련 셈플에 대한 예측값을 실행한 후에 \n",
    "그 결과를 이용하여 그레이디언트를 계산하고 파라미터를 조정한다.\n",
    "\n",
    "샘플은 무작위로 선택된다.\n",
    "따라서 경우에 따라 하나의 에포크에서 여러 번 선택되거나 전혀 선택되지 않는 샘플이\n",
    "존재할 수도 있지만, 이는 별 문제가 되지 않는다.\n",
    "\n",
    "확률적 경사 하강법<font size=\"2\">stochastic graidient descent</font>(SGD)을 이용하면 \n",
    "계산량이 상대적으로 적어 아주 큰 훈련 세트를 다룰 수 있으며,\n",
    "따라서 외부 메모리(out-of-core) 학습에 활용될 수 있다.\n",
    "또한 파라미터 조정이 불안정하게 이뤄질 수 있기 때문에 지역 최솟값에 상대적으로 덜 민감하다.\n",
    "반면에 동일한 이유로 경우에 따라 전역 최솟값에 수렴하지 못하고 주변을 맴돌 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9763d7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-04c.png\" width=\"300\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de2b0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "아래 그림은 처음 20 단계 동안의 SGD 학습 과정을 보여주는데, 모델이 수렴하지 못함을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85daf6f4",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-04d.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cfbe8b",
   "metadata": {},
   "source": [
    "**독립 항등 분포**\n",
    "\n",
    "확률적 경사 하강법을 적용하려면 훈련셋이 \n",
    "독립 항등 분포<font size=\"2\">independently and identically distributed</font>(iid)를 따르도록 해야 한다.\n",
    "이를 위해 매 에포크마다 훈련 셋을 무작위로 섞는 방법이 일반적으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebf638",
   "metadata": {
    "colab_type": "text",
    "id": "50b_hRZTRMW6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**학습 스케줄<font size=\"2\">learning schedule</font>**\n",
    "\n",
    "요동치는 파라미터를 제어하기 위해 학습률을 학습 과정 동안 천천히 줄어들게 만드는 기법을 의미한다.\n",
    "일반적으로 훈련이 지속될 수록 학습률을 조금씩 줄이며,\n",
    "에포크 수, 훈련 샘플 수, 학습되는 샘플의 인덱스를 이용하여 지정한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde60cf5",
   "metadata": {
    "colab_type": "text",
    "id": "oVXwxMY-SimN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**사이킷런의 `SGDRegressor`**\n",
    "\n",
    "확률적 경사 하강법을 기본적으로 지원한다.\n",
    "\n",
    "```python\n",
    "SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n",
    "             n_iter_no_change=100, random_state=42)\n",
    "```\n",
    "\n",
    "* `max_iter=1000`: 최대 에포크 수\n",
    "* `tol=1e-3`: 허용오차\n",
    "* `eta0=0.1`: 학습 스케줄 함수에 사용되는 매개 변수. 일종의 학습률.\n",
    "* `penalty=None`: 규제 사용 여부 결정(추후 설명). 여기서는 사용하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20870feb",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### 미니 배치 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901355f5",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "배치 크기가 2에서 수백 사이로 정해지며, 최적 배치 크기는 경우에 따라 다르다.\n",
    "\n",
    "배치 크기를 어느 정도 크게 하면 확률적 경사 하강법(SGD) 보다 파라미터의 움직임이 덜 불규칙적이 되며,\n",
    "배치 경사 하강법보다 빠르게 학습한다.\n",
    "\n",
    "반면에 SGD에 비해 지역 최솟값에 수렴할 위험도가 보다 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd85ef0",
   "metadata": {
    "colab_type": "text",
    "id": "6IvmA4ZvU3EJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**경사 하강법 비교**\n",
    "\n",
    "배치 GD, 미니 배치 GD, SGD의 순서대로 최적의 파라미터 값에 \n",
    "수렴할 확률이 높다.\n",
    "훈련 시간 또한 동일한 순서대로 오래 걸린다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ba645",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-05.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afc183",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**선형 회귀 알고리즘 비교**\n",
    "\n",
    "| 알고리즘   | 많은 샘플 수 | 외부 메모리 학습 | 많은 특성 수 | 하이퍼 파라미터 수 | 스케일 조정 | 사이킷런 지원 |\n",
    "|:--------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n",
    "| 정규방정식  | 빠름       | 지원 안됨      |  느림        | 0          | 불필요    | 지원 없음      |\n",
    "| SVD      | 빠름       | 지원 안됨      |  느림        | 0          | 불필요     | LinearRegression     |\n",
    "| 배치 GD   | 느림       | 지원 안됨      |  빠름        | 2          | 필요      | (?)      |\n",
    "| SGD      | 빠름       | 지원          |  빠름        | >= 2       | 필요      | SGDRegressor |\n",
    "| 미니배치 GD | 빠름       | 지원         |  빠름        | >=2        | 필요      | 지원 없음      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8736fd",
   "metadata": {},
   "source": [
    "**참고:** 심층 신경망을 지원하는 텐서플로우<font size=\"2\">Tensorflow</font>는 \n",
    "기본적으로 미니 배치 경사 하강법을 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35528b09",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f850604",
   "metadata": {
    "colab_type": "text",
    "id": "wni6v8aeWSI9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "비선형 데이터를 선형 회귀를 이용하여 학습하는 기법을\n",
    "**다항 회귀**<font size=\"2\">polynomial regression</font>라 한다.\n",
    "이때 다항식을 이용하여 새로운 특성을 생성하는 아이디어를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46124512",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**2차 함수 모델를 따르는 데이터셋에 선형 회귀 모델 적용 결과**\n",
    "\n",
    "아래 그림은 2차 함수의 그래프 형식으로 분포된 데이터셋을 선형 회귀 모델로 학습시킨 결과를 보여준다.\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\, x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67010cb8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-06.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449d277",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**2차 함수 모델를 따르는 데이터셋에 2차 다항식 모델 적용 결과**\n",
    "\n",
    "반면에 아래 그림은 $x_1^2$에 해당하는 특성을 새로이 추가한 후에 선형 회귀 모델을 학습시킨 결과를 보여준다.\n",
    "\n",
    "$$\\hat y = \\theta_0 + \\theta_1\\, x_1 + \\theta_2\\, x_1^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f96a02",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-07.png\" width=\"600\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58b0e7",
   "metadata": {
    "colab_type": "text",
    "id": "XC9ZZdvUXjkH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**사이킷런의 `PolynomialFeatures` 변환기**\n",
    "\n",
    "주어진 다항식에 포함된 항목에 해당하는 특성들을 생성하는 변환기이다.\n",
    "\n",
    "* `degree=d`: 몇 차 다항식을 활용할지 지정하는 하이퍼파라미터\n",
    "\n",
    "```python\n",
    "PolynomialFeatures(degree=2, include_bias=False)\n",
    "```\n",
    "\n",
    "예를 들어, $n=2, d=3$인 경우에 $(x_1+x_2)^2$과 $(x_1+x_2)^3$의 항목에 해당하는 7개 특성 추가\n",
    "\n",
    "$$x_1^2,\\,\\, x_1 x_2,\\,\\, x_2^2,\\,\\, x_1^3,\\,\\, x_1^2 x_2,\\,\\, x_1 x_2^2,\\,\\, x_2^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79f19b",
   "metadata": {
    "colab_type": "text",
    "id": "Mt1fZDkqcCSM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 학습 곡선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81881a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**예제: 선형 모델, 2차 다항 회귀 모델, 300차 다항 회귀 모델 비교**\n",
    "\n",
    "다항 회귀 모델의 차수에 따라 훈련된 모델이 훈련 세트에 과소 또는 과대 적합할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c2926",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-08.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e13b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**교차 검증 vs. 학습 곡선**\n",
    "\n",
    "* 교차 검증(2장)\n",
    "    * 과소적합: 훈련 세트와 교차 검증 점수 모두 낮은 경우\n",
    "    * 과대적합: 훈련 세트에 대한 검증은 우수하지만 교차 검증 점수가 낮은 경우\n",
    "\n",
    "* 학습 곡선 살피기\n",
    "    * 학습 곡선: 훈련 세트와 검증 세트에 대한 모델 성능을 비교하는 그래프\n",
    "    * 학습 곡선의 모양에 따라 과소적합/과대적합 판정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeae0f1",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**과소적합 모델의 학습 곡선 특징**\n",
    "\n",
    "* 훈련 데이터(빨강)에 대한 성능\n",
    "    * 훈련 세트가 커지면서 RMSE(평균 제곱근 오차)가 커짐\n",
    "    * 훈련 세트가 어느 정도 커지면 더 이상 RMSE가 변하지 않음\n",
    "\n",
    "* 검증 데이터(파랑)에 대한 성능\n",
    "    * 검증 세트에 대한 성능이 훈련 세트에 대한 성능과 거의 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871af1a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-09.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40625c55",
   "metadata": {
    "colab_type": "text",
    "id": "kWg4z3s7f_Iu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**과대적합 모델의 학습 곡선 특징**\n",
    "\n",
    "* 훈련 데이터(빨강)에 대한 성능: 훈련 데이터에 대한 평균 제곱근 오차가 매우 낮음.\n",
    "\n",
    "* 검증 데이터(파랑)에 대한 성능: 훈련 데이터에 대한 성능과 차이가 크게 벌어짐.\n",
    "\n",
    "* 과대적합 모델 개선법: 두 그래프가 맞닿을 때까지 훈련 데이터 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af20722",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-10.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62842c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**편향 vs 분산**\n",
    "\n",
    "* 편향(bias)\n",
    "    - 실제로는 2차원 모델인데 1차원 모델을 사용하는 경우처럼 잘못된 가정으로 인해 발생.\n",
    "    - 과소적합 발생 가능성 높음.\n",
    "\n",
    "* 분산(variance)\n",
    "    - 모델이 훈련 데이터에 민감하게 반응하는 정도\n",
    "    - 고차 다항 회귀 모델의 경우 분산이 높아질 수 있음.\n",
    "    - 일반적으로 **자유도**<font size='2'>degree of freedom</font>가 높은 모델일 수록 분산이 커짐.\n",
    "    - 과대적합 발생 가능성 높음.\n",
    "\n",
    "* 편향과 분산의 트레이드 오프\n",
    "    - 복잡한 모델일 수록 편향을 줄어들지만 분산을 커짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a647d58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**모델 일반화 오차의 종류**\n",
    "\n",
    "훈련 후에 새로운 데이터 대한 예측에서 발생하는 오차를 가리키며 세 종류의 오차가 있다.\n",
    "\n",
    "- 편향\n",
    "- 분산\n",
    "- 줄일 수 없는 오차\n",
    "    - 데이터 자체가 갖고 있는 잡음(noise) 때문에 발생.\n",
    "    - 잡음을 제거해야 오차를 줄일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586951f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 규제 사용 선형 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb18c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**자유도와 규제**\n",
    "\n",
    "* 자유도(degree of freedom): 학습 모델 결정에 영향을 주는 요소(특성)들의 수\n",
    "    * 단순 선형 회귀의 경우: 특성 수\n",
    "    * 다항 선형 회귀 경우: 차수\n",
    "\n",
    "* 규제(regularization): 자유도 제한\n",
    "    * 단순 선형 회귀 모델에 대한 규제: 가중치 역할 제한\n",
    "    * 다항 선형 회귀 모델에 대한 규제: 차수 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32325e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**가중치를 규제하는 선형 회귀 모델**\n",
    "\n",
    "* 릿지 회귀\n",
    "\n",
    "* 라쏘 회귀\n",
    "\n",
    "* 엘라스틱넷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddcb32",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**규제 적용 주의사항**\n",
    "\n",
    "규제항은 훈련 과정에만 사용된다. 테스트 과정에는 다른 기준으로 성능을 평가한다.\n",
    "\n",
    "* 훈련 과정: 비용 최소화 목표\n",
    "\n",
    "* 테스트 과정: 최종 목표에 따른 성능 평가\n",
    "    * 예제: 분류기의 경우 재현율/정밀도 기준으로 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef747bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 릿지 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f7753",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + \\alpha \\sum_{i=1}^{n}\\theta_i^2$$\n",
    "\n",
    "* $\\alpha$(알파): 규제 강도 지정. \n",
    "    $\\alpha=0$이면 규제가 전혀 없는 기본 선형 회귀\n",
    "\n",
    "* $\\alpha$가 커질 수록 가중치의 역할이 줄어듦. \n",
    "    비용을 줄이기 위해 가중치를 작게 유지하는 방향으로 학습\n",
    "\n",
    "* $\\theta_0$은 규제하지 않음\n",
    "\n",
    "* 주의사항: 특성 스케일링 전처리를 해야 성능이 좋아짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa374f48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 라쏘 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e34e9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + 2 \\alpha \\, \\sum_{i=1}^{n}\\mid \\theta_i\\mid$$\n",
    "\n",
    "* $\\alpha$(알파): 규제 강도 지정.\n",
    "    $\\alpha=0$이면 규제가 전혀 없는 기본 선형 회귀\n",
    "\n",
    "* $\\theta_i$: 덜 중요한 특성을 무시하기 위해 $\\mid\\theta_i\\mid$가 0에 수렴하도록 학습 유도.\n",
    "\n",
    "* $\\theta_0$은 규제하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a5587",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 엘라스틱 넷"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df25b67",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수\n",
    "\n",
    "$$J(\\theta) = \\textrm{MSE}(\\theta) + 2r\\, \\alpha \\, \\sum_{i=1}^{n}\\mid\\theta_i\\mid + \\,(1-r)\\, \\alpha\\, \\sum_{i=1}^{n}\\theta_i^2$$\n",
    "\n",
    "* 릿지 회귀와 라쏘 회귀를 절충한 모델\n",
    "\n",
    "* 혼합 비율 $r$을 이용하여 릿지 규제와 라쏘 규제를 적절하게 조절"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456c12b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**규제 선택**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ba6db",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 대부분의 경우 약간이라도 규제 사용 추천\n",
    "\n",
    "* 릿지 규제가 기본\n",
    "\n",
    "* 유용한 속성이 많지 않다고 판단되는 경우 \n",
    "    * 라쏘 규제나 엘라스틱넷 활용 추천\n",
    "    * 불필요한 속성의 가중치를 0으로 만들기 때문\n",
    "\n",
    "* 특성 수가 훈련 샘플 수보다 크거나 특성 몇 개가 강하게 연관되어 있는 경우\n",
    "    * 라쏘 규제는 적절치 않음.\n",
    "    * 엘라스틱넷 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b176901",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 조기 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15dbcbf",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "조기 종료는 모델의 훈련 세트에 과대 적합하는 것을 방지하기 위해 훈련을 적절한 시기에 중단시키는 기법이며,\n",
    "검증 데이터에 대한 손실이 줄어 들다가 다시 커지는 순간 훈련을 종료한다. \n",
    "\n",
    "확률적 경사 하강법, 미니 배치 경사 하강법에서는 손실 곡선이 보다 많이 진동하기에\n",
    "검증 손실이 언제 최소가 되었는지 알기 어렵다.\n",
    "따라서 한동안 최솟값보다 높게 유지될 때 훈련을 멈추고 기억해둔 최소 검증 손실 모델로\n",
    "되돌린다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2765a7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-11.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980e759",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd1c9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "회귀 모델을 분류 모델로 활용할 수 있다. \n",
    "\n",
    "* 이진 분류: 로지스틱 회귀\n",
    "\n",
    "* 다중 클래스 분류: 소프트맥스 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f6b0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 확률 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdbbc5a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 시그모이드 함수\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55a70a",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-12.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10eded",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 로지스틱 회귀 모델에서 샘플 $\\mathbf x$가 양성 클래스에 속할 확률\n",
    "\n",
    "$$\\hat p = h_\\theta (\\mathbf x)\n",
    "= \\sigma(\\theta_0 + \\theta_1\\, x_1 + \\cdots + \\theta_n\\, x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88c12d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**예측값**\n",
    "\n",
    "$$\n",
    "\\hat y = \n",
    "\\begin{cases}\n",
    "0 & \\text{if}\\,\\, \\hat p < 0.5 \\\\\n",
    "1 & \\text{if}\\,\\, \\hat p \\ge 0.5\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e289ae",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 양성 클래스인 경우: \n",
    "\n",
    "$$\\theta_0 + \\theta_1\\, x_1 + \\cdots + \\theta_n\\, x_n \\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f2d34",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 음성 클래스인 경우: \n",
    "\n",
    "$$\\theta_0 + \\theta_1\\, x_1 + \\cdots + \\theta_n\\, x_n < 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c15f8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 훈련과 비용함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7f6b3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 비용함수: 로그 손실(log loss) 함수 사용\n",
    "\n",
    "$$\n",
    "J(\\theta) = \n",
    "- \\frac{1}{m}\\, \\sum_{i=1}^{m}\\, [y^{(i)}\\, \\log(\\,\\hat p^{(i)}\\,) + (1-y^{(i)})\\, \\log(\\,1 - \\hat p^{(i)}\\,)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0799fd97",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 모델 훈련: 위 비용함수에 대해 경사 하강법 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c5d48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**로그 손실 함수 이해**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b05699",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 틀린 예측을 하면 손실값이 많이 커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1c6c9",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$$\n",
    "- [y\\, \\log(\\,\\hat p\\,) + (1-y)\\, \\log(\\,1 - \\hat p\\,)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275b854",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-12-10a.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8abe5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**로그 손실 함수의 편도 함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd5b0e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{\\partial}{\\partial \\theta_j} \\text{J}(\\boldsymbol{\\theta}) = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\left(\\mathbf{\\sigma(\\boldsymbol{\\theta}}^T \\mathbf{x}^{(i)}) - y^{(i)}\\right)\\, x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd511a",
   "metadata": {},
   "source": [
    "* 편도 함수가 선형 회귀의 경우와 매우 비슷한 것에 대한 확률론적 근거가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f08b02",
   "metadata": {},
   "source": [
    "* __참고:__ [앤드류 응(Andrew Ng) 교수의 Stanford CS229](https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f060ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 결정 경계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f080b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**예제: 붓꽃 데이터셋**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0730a42",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 꽃받침(sepal)과 꽃입(petal)과 관련된 4개의 특성 사용\n",
    "    * 꽃받침 길이\n",
    "    * 꽃받침 너비\n",
    "    * 꽃잎 길이\n",
    "    * 꽃잎 너비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9fb03",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 타깃: 세 개의 품종\n",
    "    * 0: Iris-Setosa(세토사)\n",
    "    * 1: Iris-Versicolor(버시컬러)\n",
    "    * 2: Iris-Virginica(버지니카)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1bc06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**꽃잎의 너비를 기준으로 Iris-Virginica 여부 판정하기**\n",
    "\n",
    "* 결정경계: 약 1.6cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32161b40",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-14.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b398ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**꽃잎의 너비와 길이를 기준으로 Iris-Virginica 여부 판정하기**\n",
    "\n",
    "* 결정경계: 검정 점선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002916e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-15.png\" width=\"700\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cde1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**로지스틱 회귀 규제하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f134d3c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 하이퍼파라미터 `penalty`와 `C` 이용\n",
    "\n",
    "* `penalty`\n",
    "    * `l1`, `l2`, `elasticnet` 세 개중에 하나 사용.\n",
    "    * 기본은 `l2`, 즉, $\\ell_2$ 규제를 사용하는 릿지 규제.\n",
    "    * `elasticnet`을 선택한 경우 `l1_ration` 옵션 값을 함께 지정.\n",
    "\n",
    "* `C`\n",
    "    * 릿지 또는 라쏘 규제 정도를 지정하는 $\\alpha$의 역수에 해당. \n",
    "    * 따라서 0에 가까울 수록 강한 규제 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13540217",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 소프트맥스(softmax) 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45dc3f5",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 로지스틱 회귀 모델을 일반화하여 다중 클래스 분류를 지원하도록 한 회귀 모델\n",
    "\n",
    "* **다항 로지스틱 회귀** 라고도 불림\n",
    "\n",
    "* 주의사항: 소프트맥스 회귀는 다중 출력 분류 지원 못함. \n",
    "    예를 들어, 하나의 사진에서 여러 사람의 얼굴 인식 불가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9094eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**소프트맥스 회귀 학습 아이디어**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f47d69",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 샘플 $\\mathbf x$가 주어졌을 때 각각의 분류 클래스 $k$ 에 대한 점수 $s_k(\\mathbf x)$ 계산.\n",
    "    즉, `k*(n+1)` 개의 파라미터를 학습시켜야 함.\n",
    "\n",
    "$$\n",
    "s_k(\\mathbf x) = \\theta_0^{(k)} + \\theta_1^{(k)}\\, x_1 + \\cdots + \\theta_n^{(k)}\\, x_n\n",
    "$$    \n",
    "\n",
    "* __소프트맥스 함수__를 이용하여 각 클래스 $k$에 속할 확률 $\\hat p_k$ 계산\n",
    "\n",
    "$$\n",
    "\\hat p_k = \n",
    "\\frac{\\exp(s_k(\\mathbf x))}{\\sum_{j=1}^{K}\\exp(s_j(\\mathbf x))}\n",
    "$$\n",
    "\n",
    "* 추정 확률이 가장 높은 클래스 선택\n",
    "\n",
    "$$\n",
    "\\hat y = \n",
    "\\mathrm{argmax}_k s_k(\\mathbf x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de7152",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**소프트맥스 회귀 비용함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eead2e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "* 각 분류 클래스 $k$에 대한 적절한 가중치 벡터 $\\theta_k$를 학습해 나가야 함.\n",
    "\n",
    "* 비용함수: 크로스 엔트로피 비용 함수 사용\n",
    "\n",
    "$$\n",
    "J(\\Theta) = \n",
    "- \\frac{1}{m}\\, \\sum_{i=1}^{m}\\sum_{k=1}^{K} y^{(i)}_k\\, \\log(\\hat{p}_k^{(i)})\n",
    "$$\n",
    "\n",
    "* 위 비용함수에 대해 경사 하강법 적용\n",
    "\n",
    "* $K=2$이면 로지스틱 회귀의 로그 손실 함수와 정확하게 일치.\n",
    "\n",
    "* 주어진 샘플의 타깃 클래스를 제대로 예측할 경우 높은 확률값 계산\n",
    "\n",
    "* 크로스 엔트로피 개념은 정보 이론에서 유래함. 자세한 설명은 생략."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ecb14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**다중 클래스 분류 예제**\n",
    "\n",
    "* 사이킷런의 `LogisticRegression` 예측기 활용\n",
    "    * `multi_class=multinomial`로 지정\n",
    "    * `solver=lbfgs`: 다중 클래스 분류 사용할 때 반드시 지정\n",
    "\n",
    "* 붓꽃 꽃잎의 너비와 길이를 기준으로 품종 분류\n",
    "    * 결정경계: 배경색으로 구분\n",
    "    * 곡선: Iris-Versicolor 클래스에 속할 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc91c8",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch04/homl04-16.png\" width=\"700\"/></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
