

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>9. 비지도 학습 &#8212; 핸즈온 머신러닝(3판)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'unsupervised_learning';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="8. 차원 축소" href="dimensionality_reduction.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    <p class="title logo__title">핸즈온 머신러닝(3판)</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ml_landscape.html">1. 한눈에 보는 머신러닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="end2end_ml_project.html">2. 머신러닝 프로젝트 처음부터 끝까지</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification.html">3. 분류</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_models.html">4. 모델 훈련</a></li>
<li class="toctree-l1"><a class="reference internal" href="svm.html">5. 서포트 벡터 머신</a></li>
<li class="toctree-l1"><a class="reference internal" href="decision_trees.html">6. 결정트리</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensemble_learning_random_forests.html">7. 앙상블 학습과 랜덤 포레스트</a></li>
<li class="toctree-l1"><a class="reference internal" href="dimensionality_reduction.html">8. 차원 축소</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">9. 비지도 학습</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/jupyter-book/unsupervised_learning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/codingalzi/handson-ml3" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="소스 저장소"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/codingalzi/handson-ml3/issues/new?title=Issue%20on%20page%20%2Funsupervised_learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="이슈 열기"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="이 페이지 다운로드">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/unsupervised_learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="소스 파일 다운로드"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="PDF로 인쇄"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="전체 화면으로보기"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>비지도 학습</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> 내용 </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">9.1. 분류 대 군집화</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k">9.2. k-평균</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">9.2.1. k-평균 알고리즘</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">9.2.2. 센트로이드 초기화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">9.2.3. 최적의 군집수</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">9.2.4. k-평균의 한계</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">9.3. 군집화 활용</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">9.3.1. 이미지 분할</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">9.3.2. 준지도 학습</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">9.3.3. 레이블 전파</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dbscan">9.4. DBSCAN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">9.5. 가우스 혼합 모델</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">9.5.1. 가우스 혼합 모델 활용: 이상치 탐지</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">9.5.2. 가우션 혼합모델 군집수 지정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">9.5.3. 베이즈 가우스 혼합 모델</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">9.5.4. 이상치 탐지와 특이치 탐지를 위한 다른 알고리즘</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">9.6. 연습문제</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ch-unsupervisedlearning">
<span id="id1"></span><h1><span class="section-number">9. </span>비지도 학습<a class="headerlink" href="#ch-unsupervisedlearning" title="Permalink to this headline">#</a></h1>
<p><strong>감사의 글</strong></p>
<p>자료를 공개한 저자 오렐리앙 제롱과 강의자료를 지원한 한빛아카데미에게 진심어린 감사를 전합니다.</p>
<p><strong>소스코드</strong></p>
<p>본문 내용의 일부를 파이썬으로 구현한 내용은
<a class="reference external" href="https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_unsupervised_learning.ipynb">(구글코랩) 비지도 학브</a>에서 확인할 수 있다.</p>
<p><strong>슬라이드</strong></p>
<p>본문 내용을 요약한
<a class="reference external" href="https://github.com/codingalzi/handson-ml3/raw/master/slides/slides-unsupervised_learning-1.pdf">슬라이드 1부</a>와
<a class="reference external" href="https://github.com/codingalzi/handson-ml3/raw/master/slides/slides-unsupervised_learning-2.pdf">슬라이드 2부</a>를
다운로드할 수 있다.</p>
<p><strong>소개</strong></p>
<p>비지도 학습은 레이블이 없는 데이터를 학습하는 기법이다.
<a class="reference internal" href="dimensionality_reduction.html#ch-dimensionalityreduction"><span class="std std-numref">8장</span></a>에서 다룬 차원 축소 기법도 비지도 학습의 전형적인 예제이다.
여기서는 다음 주제와 관련된 비지도 학습을 다룬다.</p>
<ul class="simple">
<li><p>군집화: 비슷한 샘플끼리의 군집 형성</p>
<ul>
<li><p>고객 분류</p></li>
<li><p>추천 시스템</p></li>
<li><p>데이터 분석</p></li>
<li><p>차원 축소</p></li>
<li><p>특성 공학</p></li>
<li><p>준지도 학습</p></li>
<li><p>검색 엔진</p></li>
<li><p>이미지 분할</p></li>
</ul>
</li>
<li><p>이상치 탐지: 정상 테이터와 이상치 구분</p>
<ul>
<li><p>생산라인에서 결함제품 탐지</p></li>
<li><p>새로운 트렌드 찾기</p></li>
</ul>
</li>
<li><p>데이터 밀도 추정: 데이터셋의 확률 밀도를 추정</p>
<ul>
<li><p>이상치 분류</p></li>
<li><p>데이터 시각화</p></li>
<li><p>데이터 분석</p></li>
</ul>
</li>
</ul>
<section id="id2">
<h2><span class="section-number">9.1. </span>분류 대 군집화<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p><strong>군집</strong><font size='2'>cluster</font>은 유사한 대상들의 모음을 가리킨다.
예를 들어, 산이나 공원에서 볼 수 있는 이름 모르는 동일 품종의 꽃으로 이루어진 군집을 생각할 수 있다.
<strong>군집화</strong><font size='2'>clustering</font>는 뭔지 모를 어떤 기준으로 대상을 여러 개의 군집으로
나누는 과정을 가리킨다.</p>
<p>분류와 군집화는 각 샘플에 하나의 그룹을 할당한다는 점에서 유사하다.
하지만 분류는 미리 지정된 레이블(타깃)을 최대한 정확하게 예측하는 과정을 의미하는 반면에,
군집화는 미리 지정된 레이블(타깃)이 없음에도 불구하고 예측기 스스로 찾아낸 특정 기준을 이용해서
여러 개의 군집으로 나누는 과정을 가리킨다.</p>
<p>다음 세 종류의 군집화 알고리즘을 자세히 소개한다.
알고리즘에 따라 생성되는 군집의 특성과 모양이 다르다.</p>
<ul class="simple">
<li><p>k-평균: 센트로이드(중심)라는 특정 샘플을 중심으로 모인 샘플들의 집합</p></li>
<li><p>DBSCAN: 밀집된 샘플들의 연속으로 이루어진 집합</p></li>
<li><p>가우스 혼합: 특정 가우스 분포를 따르는 샘플들의 집합</p></li>
</ul>
<p>이외에 군집의 군집 등 다양한 군집의 모양과 특성이 존재한다.</p>
<p><strong>예제: 붓꽃 데이터셋 군집화</strong></p>
<p>아래 왼쪽 그림은 붓꽃의 꽃잎 길이와 너비를 특성으로 사용해서 품종을 분류한 결과를
보여주지만, 오른쪽 그림은 어떤 품종인지는 모르지만
노랑 동그라미아 검정 별표로 구분된 두 품종의 군집을 보여준다.
분류는 세 개의 품종을 매우 잘 분류하지만 군집은 세토사 군집과 나머지 군집으로 구분할 뿐이다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-01.png" width="80%"/></div><p>반면에 <strong>가우스 혼합 모델</strong><font size='2'>Gaussian Mixture Model</font>(GMM)을
꽃잎의 길이와 너비 뿐만 아니라
꽃받침의 길이와 너비 특성까지 특성으로 사용하는 붓꽃 데이터셋에 대해 적용하면
세 개의 군집을 매우 정확하게 생성한다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-02.png" width="450"/></div></section>
<section id="k">
<h2><span class="section-number">9.2. </span>k-평균<a class="headerlink" href="#k" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>참고: <a class="reference external" href="https://codingalzi.github.io/handson-ml3/end2end_ml_project.html#id18">2장</a>에서 사용된 <code class="docutils literal notranslate"><span class="pre">ClusterSimilarity</span></code> 클래스 다시 활용할 것.</p></li>
</ul>
<p>각 군집의 중심인 센트로이드<font size='2'>centroid</font>을 찾고
각 샘플에 대해 가장 가까운 센트로이드를 중심으로 군집을 형성하는 기법이다.</p>
<p><strong>사이킷런의 <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> 모델</strong></p>
<p>아래 그림은 다섯 개의 샘플 덩어리로 이루어진 데이터셋을 보여준다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-03.png" width="450"/></div><p>위 데이터셋에 대해 다섯 개의 군집을 형성하는 k-평균 알고리즘은 다음과 같이 적용한다.
군집 수를 몇 개로 지정하는 게 가장 좋은지는 미리 알 수 없다.
나중에 몇 개의 군집이 적절한가를 판단하는 여러 방식을 살펴볼 것이다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>예측값</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">predict()</span></code> 함수의 반환값은 0, 1, 2, 3, 4 등 정수로 구성된다.
하지만 이는 임의로 지정된 군집의 인덱스를 가리킬 뿐이며 클래스 분류와는 아무 상관 없다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span>
<span class="go">array([4, 0, 1, ..., 2, 1, 0])</span>
</pre></div>
</div>
<p><strong>센트로이드 정보</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">KMeans</span></code> 모델이 찾아낸 센트로이드 정보는 <code class="docutils literal notranslate"><span class="pre">cluster_centers_</span></code> 속성에 저장된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="go">array([[-2.80389616,  1.80117999],</span>
<span class="go">       [ 0.20876306,  2.25551336],</span>
<span class="go">       [-2.79290307,  2.79641063],</span>
<span class="go">       [-1.46679593,  2.28585348],</span>
<span class="go">       [-2.80037642,  1.30082566]])</span>
</pre></div>
</div>
<p><strong>보로노이 다이어그램</strong></p>
<p><strong>보로노이 다이어그램</strong><font size='2'>Voronoi diagram</font>은
평면을 특정 점(센트로이드)까지의 거리가 가장 가까운 점들의 집합으로 분할한 그림이다.
점들이 군집을 잘 구성하는지 여부를 쉽게 확인할 수 있다.</p>
<img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-04.png" width="450"/><p>왼쪽 상단 군집에 포함된 샘플들 중에서  군집 경계 근처에 있는 샘플들의 군집이 잘못 지정됐다.
이유는 그 오른편에 위치한 군집의 직경이 보다 크기에 사실
그 군집에 속해야 하는 샘플이 왼쪽 센트로이드와의 거리가 단지 보다 가깝다는 이유로 왼쪽 군집으로 판정되었다.
이렇듯 군집의 직경이 서로 많이 다르면 군집화가 잘 작동하지 않을 수 있다.</p>
<p><strong>하드 군집화 대 소프트 군집화</strong></p>
<p>지금까지 살펴 보았듯이 k-평균 모델 객체의 labels_ 속성은 각 샘플에 대해 가장 가까운 센트로이드를 중심으로 하는 군집의 (작위적으로 지정된) 인덱스를 저장하며, 이를 이용하여 predict() 메서드는 샘플이 속하는 군집의 인덱스를 반환한다. 이런 방식의 군집화가 <strong>하드 군집화</strong>(hard clustering)이다.</p>
<p>반면에 <strong>소프트 군집화</strong>(soft clustering)는 샘플과 각 군집 사이의 관계를 점수로 부여한다. 점수는 예를 들어 각 군집과 샘플사이의 거리 또는 5장에서 소개한 가우시안 방사기저 함수를 이용한 유사도 점수 등이 사용될 수 있다. k-평균 모델 객체의 transform() 메서드는 샘플과 각 센트로이드 사이의 (유클리드) 거리를 점수로 사용한다.</p>
<p>아래 코드는 다섯 개의 특성을 갖는 새로운 데이터로 변환하는 것을 보여준다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">array([[2.81, 0.33, 2.9 , 1.49, 2.89],</span>
<span class="go">       [5.81, 2.8 , 5.85, 4.48, 5.84],</span>
<span class="go">       [1.21, 3.29, 0.29, 1.69, 1.71],</span>
<span class="go">       [0.73, 3.22, 0.36, 1.55, 1.22]])</span>
</pre></div>
</div>
<section id="id3">
<h3><span class="section-number">9.2.1. </span>k-평균 알고리즘<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<p>먼저 몇 개의 군집으로 분류할지를 정하기 위해 k 값을 지정한다.
그런 다음 k 개의 센트로이드를 무작위로 선택한 다음에
센트로이드들의 위치가 수렴할 때까지 아래 과정을 반복한다.</p>
<ul class="simple">
<li><p>각 샘플을 가장 가까운 센트로이드에 할당한다.</p></li>
<li><p>군집별로 샘플의 평균을 계산하여 새로운 해당 군집의 센트로이드로 지정한다.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-05.png" width="700"/></div><p><strong>무작위 초기화 문제</strong></p>
<p>임의로 선택된 초기 센트로이드에 따라 매우 다른 모양과 성질의 군집이 생성될 수 있다.
아래 오른쪽 그림은 센트로이드 초기화가 다를 경우 최종 결과도 많이 다를 수 있음을 잘 보여준다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-06.png" width="750"/></div></section>
<section id="id4">
<h3><span class="section-number">9.2.2. </span>센트로이드 초기화<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h3>
<p><strong>관성</strong></p>
<p><strong>관성</strong><font size='2'>intertia</font>은 각 샘플과 가장 가까운 센트로이드와의 거리의 제곱의 합이며,
각 군집이 센트로이드에 얼마나 가까이 모여있는가를 측정한다.
따라서 관성이 작을 수록 군집이 잘 구성되었다고 평가한다.</p>
<p>훈련된 KMeans 모델의 경우 <code class="docutils literal notranslate"><span class="pre">inertia_</span></code> 속성에 관성 값이 저징되며,
<code class="docutils literal notranslate"><span class="pre">score()</span></code> 메서드가 관성의 음숫값을 반환한다.
이유는 점수(score)는 높을 수록 좋은 모델을 나타내도록 해야 하기 때문이다.
KMeans 모델은 훈련 과정 중에 다양한 초기화 과정을 실험하고 그 중에
관성이 가장 작은 값이 되는 센트로이드를 선택한다.</p>
<p><strong>센트로이드 초기화 반복 횟수</strong></p>
<p>무작위 초기와 문제를 해결하기 위해 k-평균 알고리즘의 초기화를 여러 번 실행한 다음에 가장 낮은
관성을 보이는 모델을 최종 모델로 선택한다.
이전 코드에서 <code class="docutils literal notranslate"><span class="pre">n_init=10</span></code>으로 지정되어 있어서 센트로이드 초기화를 10번 진행한다.</p>
<p><strong>k-평균++ 초기화 알고리즘</strong></p>
<p>센트로이드 무작위 초기화 문제의 보다 근본적인 해결책이
아서(David Arthur)와 바실비츠키(Sergei Vassilvitskii)의 논문
<a class="reference external" href="https://www.semanticscholar.org/paper/k-means%2B%2B%3A-the-advantages-of-careful-seeding-Arthur-Vassilvitskii/5e0c61b7ee4a2de183a197f32c5013ad109531fa">k-means++: the advantages of careful seeding</a>에서 제시되었다.</p>
<p>k-평균++ 초기화 알고리즘은 기존에 선택된
센트로이드들과의 거리가 먼 샘플일 수록 다음 센트로이드로 선택될 확률이 높아지도록 한다.
보다 구체적으로 다음 과정을 따른다.</p>
<ol class="arabic">
<li><p>임의로 하나의 센트로이드 <span class="math notranslate nohighlight">\(c_1\)</span> 선택 후 <span class="math notranslate nohighlight">\(k\)</span> 개의 센트로이드를 지정할 때까지 아래 과정을 반복한다.</p></li>
<li><p><span class="math notranslate nohighlight">\(c_1, \dots, c_{i-1}\)</span>이 이미 선택되었가고 가정했을 대,
각 샘플  <span class="math notranslate nohighlight">\(\mathbf{x}_j\)</span>가 아래의 확률로 새로운 센트로이드 <span class="math notranslate nohighlight">\(c_i\)</span>로 선택되도록 한다.</p>
<div class="math notranslate nohighlight">
\[\frac{D(\mathbf{x}_j)^2}{\sum\limits_{j=1}^{m}{D(\mathbf{x}_j)}^2}\]</div>
<p>단, <span class="math notranslate nohighlight">\(m\)</span>은 훈련셋의 크기를, <span class="math notranslate nohighlight">\(D(\mathbf{x}_j)\)</span>는 <span class="math notranslate nohighlight">\(\mathbf{x}_j\)</span>와 이미 선택된 <span class="math notranslate nohighlight">\(c_1, \dots, c_{i-1}\)</span> 중에서
가장 가까운 센트로이드 사이의 거리를 가리킨다.</p>
<div class="math notranslate nohighlight">
\[D(\mathbf{x}_j) = \min_{p&lt;i} \| x_j - c_p \|\]</div>
</li>
</ol>
<p>확률 계산으로 인해 초기화 비용이 좀 더 많이 들어가긴 하지만 결과적으로 초기화 횟수(<code class="docutils literal notranslate"><span class="pre">n_init</span></code>)를
획기적으로 줄일 수 있는 장점이 보다 크다.
따라서 사이킷런의 <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> 모델의 기본 초기화 알고리즘으로 사용된다.</p>
<p><strong>미니배치 k-평균</strong></p>
<p>미니배치를 사용해서 센트로이드를 조금씩 이동하는 k-평균 알고리즘이다.
사이킷런의 <code class="docutils literal notranslate"><span class="pre">MiniBatchMeans</span></code> 모델이 지원한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">minibatch_kmeans</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                   <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">minibatch_kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_memmap</span><span class="p">)</span>
</pre></div>
</div>
<p>군집수가 많아질 수록 k-평균보다 서너 배 정도 빠르게 훈련되지만, 성능은 조금 낮다.
하지만 아래 왼쪽 그림에서 보면 군집수 k 가 커져도 성능 차이가 유지되지만
성능 자체가 좋아지므로 두 모델의 상대적 성능 차이는 점점 벌어짐을 의미한다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-07.png" width="700"/></div><div class="info admonition">
<p class="admonition-title"><code class="docutils literal notranslate"><span class="pre">MiniBatchKMeans</span></code>와 <code class="docutils literal notranslate"><span class="pre">memmap</span></code> 클래스</p>
<p><a class="reference internal" href="dimensionality_reduction.html#ch-dimensionalityreduction"><span class="std std-numref">8장</span></a>에서 점진적 PCA를 소개하면서 언급한
넘파이 <code class="docutils literal notranslate"><span class="pre">memmap</span></code> 클래스를 이용하여 매우 큰 데이터셋을 조금씩 모델에 제공할 수 있다.</p>
</div>
</section>
<section id="id5">
<h3><span class="section-number">9.2.3. </span>최적의 군집수<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h3>
<p>군집수가 적절하지 않으면 좋지 않은 모델로 수렴할 수 있다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-08.png" width="700"/></div><p><strong>방법 1: 관성과 군집수</strong></p>
<p>군집수 k가 증가할 수록 관성은 기본적으로 줄어들기에 관성만으로 모델을 평가할 수는 없다.
하지만 관성이 더 이상 획기적으로 줄어들지 않는 지점을 군집수 후보로 선정할 수 있다.
예를 들어 아래 그래프는 k가 1부터 9까지 변하는 동안 훈련된 모델의 관성을 측정하며,
관성이 줄어드는 현상이 약화되는 k=4를 후보로 추천한다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-09.png" width="600"/></div><p>이유는 군집이 네 개보다 작으면 별로이고, 4개보다 많아도 훨씬 좋아진다고 보기 어렵기 때문이다.
하지만 4개의 군집으로 구성하려 하면 아래 그림과 같이 왼쪽 하단 두 개의 군집이 하나의 군집으로 처리될 수 있기에
가장 좋은 군집화라고 말하기 어렵다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-10.png" width="400"/></div><p><strong>방법 2: 실루엣 점수와 군집수</strong></p>
<p><strong>실루엣 점수</strong><font size='2'>silhouette score</font> 샘플별 실루엣 계수의 평균값이다.
샘플의 <strong>실루엣 계수</strong><font size='2'>silhouette coefficient</font>는
다음 식으로 계산된다.</p>
<div class="math notranslate nohighlight">
\[\frac{b - a}{\max(a, b)}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a\)</span>: 동일 군집 내의 다른 샘플들과의 거리의 평균값</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span>: 가장 가까운 타 군집에 속하는 샘플들과의 거리의 평균값</p></li>
</ul>
<p>실루엣 계수는 -1과 1사이의 값이며, 다음 특성을 보여준다.</p>
<ul class="simple">
<li><p>1에 가까운 값: 적절한 군집에 포함됨.</p></li>
<li><p>0에 가까운 값: 군집 경계에 위치</p></li>
<li><p>-1에 가까운 값: 잘못된 군집에 포함됨</p></li>
</ul>
<p>k=4가 여전히 매우 좋아 보인다.
하지만 관성의 경우와는 달리 k=5도 역시 꽤 좋다는 것을 알 수 있다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-11.png" width="600"/></div><p><strong>방법 3: 실루엣 다이어그램과 군집수</strong></p>
<p><strong>실루엣 다이어그램</strong>은 군집별로 실루엣 계수들의 모아 놓은 그래프다.
군집별로 실루엣 계수를 내림차순으로 정렬하면 칼날 모양이 형성된다.</p>
<ul class="simple">
<li><p>칼날 두께: 군집에 포함된 샘플 수</p></li>
<li><p>칼날 길이: 군집에 포함된 각 샘플의 실루엣 계수</p></li>
<li><p>빨강 파선: 실루엣 점수, 즉 실루엣 계수의 평균값이다.</p></li>
</ul>
<p>좋은 군집 모델은 대부분의 칼날이 빨간 파선보다 길어야 하며,
칼날의 두께가 서로 비슷해야 한다.
즉, 군집별 크기가 비슷해야 좋은 모델이다.
이런 기준으로 볼 때 <code class="docutils literal notranslate"><span class="pre">k=5</span></code> 가 가장 좋은 모델이다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-12.png" width="700"/></div></section>
<section id="id6">
<h3><span class="section-number">9.2.4. </span>k-평균의 한계<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h3>
<p>첫째, 최적의 모델을 구하기 위해 여러 번 학습해야 한다.</p>
<p>둘째, 군집수를 미리 지정해야 한다.</p>
<p>셋째, 군집의 크기나, 샘플의 밀도가 다르거나, 원형이 아닐 경우 잘 작동하지 않는다.
예를 들어, 아래 그림에서 사용된 데이터 샘플들의 분포가 원형이 아니다.
오른쪽의 관성이 보다 작지만 훨씬 나뿐 군집이 생성되었다.
데이터 분포가 타원형인 경우 이어서 소개하는 가우스 혼합 모델(GMM)이 매우 잘 작동한다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-13.png" width="600"/></div></section>
</section>
<section id="id7">
<h2><span class="section-number">9.3. </span>군집화 활용<a class="headerlink" href="#id7" title="Permalink to this headline">#</a></h2>
<section id="id8">
<h3><span class="section-number">9.3.1. </span>이미지 분할<a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h3>
<p>이미지 분할은 보통 다음 세 가지 중에 하나를 가리킨다.</p>
<ul class="simple">
<li><p>시맨틱 분할</p></li>
<li><p>인스턴스 분할</p></li>
<li><p>색상 분할</p></li>
</ul>
<p><strong>시맨틱 분할</strong><font size='2'>semantic segmentation</font>은
사진에 들어 있는 사물들을 클래스별로 분할한다.
예를 들어 아래 왼쪽 사진에서 배경과 구분된 고양이들을 묶어서 cat 클래스로 분류한다.
다만 고양의 종류와 개수는 따지지 않는다.</p>
<p><strong>인스턴스 분할</strong><font size='2'>instance segmentation</font>은 클래스 뿐만 아니라 객체도 분할한다.
예를 들어 아래 오른쪽 사진에서 배경과 구분된 각각의 고양이를 cat1, cat2 등으로 구별한다.</p>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/instance_segmentation.png" style="width:80%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p><strong>색상 분할</strong><font size='2'>color segmentation</font>은 유사 색상으로 이루어진 군집으로 분할하는 것을 의미한다.
아래 그림은 무당벌레가 포함된 이미지를 대상으로 색상 수를 다르게 하면서
색상 분할을 시도한 결과를 보여준다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-14.png" width="80%"/></div></section>
<section id="id9">
<h3><span class="section-number">9.3.2. </span>준지도 학습<a class="headerlink" href="#id9" title="Permalink to this headline">#</a></h3>
<p><strong>준지도 학습</strong><font size='2'>semi-supervised learning</font>은
레이블이 있는 데이터가 적고, 레이블이 없는 데이터가 많을 때 활용한다.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 9.1 </span> (미니 MNIST 데이터셋)</p>
<section class="example-content" id="proof-content">
<p>미니 MNist 데이터셋은 1,797 개의 8x8 크기의 손글씨 이미지로 구성된다.</p>
</section>
</div><p>예를 들어, 미니 MNIST 데이터셋을 50개의 군집으로 나눈 후 각 군집에서
센트로이드에 가장 가까운 샘플 50개를 대표 이미지로 선정한다.
선정된 50개 샘플만을 이용하여 분류 모델을 훈련해도 84.9%의 정확도가 달성된다.</p>
</section>
<section id="id10">
<h3><span class="section-number">9.3.3. </span>레이블 전파<a class="headerlink" href="#id10" title="Permalink to this headline">#</a></h3>
<p>대표 이미지의 레이블을 해당 군집의 모든 샘플로 전파하는
기법을 <strong>레이블 전파</strong><font size='2'>label propagation</font>라 한다.
레이블 전파를 이용하여 예를 들어 미니 MNIST 데이터셋의 50개 군집의 대표 이미지의 레이블을 각 군집의 전체 샘플에 전파한 다음에
전체 훈련셋을 대상으로 분류 모델을 훈련하면 89% 이상으로 정확도가 올라간다.</p>
<p>또한 센트로이드에 가장 멀리 떨어진 1%의 데이터를 이상치로 취급하여 각 군집에서 제외시킨
다음에 레이블 전파된 훈련셋을 이용하면 분류 모델의 성능이 조금이나마 향상된다.</p>
<p><code class="docutils literal notranslate"><span class="pre">sklearn.semi_supervised</span></code> 패키지는 다양한 레이블 전파 클래스를 제공한다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LabelSpreading</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LabelPropagation</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SelfTrainingClassifier</span></code></p></li>
</ul>
<p><strong>액티브 러닝</strong></p>
<p>모델의 성능을 보다 높이기 위한 다음 단계로
<strong>액티브 러닝</strong><font size='2'>Active Learning</font> 기법을 적용할 수 있다.
액티브 러닝은 기존에 훈련된 모델의 약점을 보완하려는 목적으로
일부 샘플을 사람이 직접 확인해서 라벨을 지정하는 과정을 모델의 성능이 더 이상
개선되지 않을 때까지 반복한다.</p>
<p>다양한 샘플 선택 전략이 알려져 있지만 <strong>불확실성 샘플링</strong><font size='2'>uncertainty sammpling</font> 전략이
가장 많이 사용되며, 아래 과정을 훈련 성능이 더 이상 개선되지 않을 때까지 반복한다.</p>
<ol class="arabic simple">
<li><p>기존에 정리된 훈련셋을 이용하여 모델을 학습시킨다.</p></li>
<li><p>훈련된 모델이 예측에 대해 가장 불확실해 하는 샘플들을 대상으로 사람이 직접 라벨을 확인한다.</p></li>
</ol>
</section>
</section>
<section id="dbscan">
<h2><span class="section-number">9.4. </span>DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>연속적인 밀집 지역을 하나의 군집으로 설정.</p></li>
</ul>
<p><strong>사이킷런의 DBSCAN 모델</strong></p>
<ul class="simple">
<li><p>두 개의 하이퍼파라미터 사용</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>: <span class="math notranslate nohighlight">\(\varepsilon\)</span>-이웃 범위</p>
<ul>
<li><p>주어진 기준값 <span class="math notranslate nohighlight">\(\varepsilon\)</span> 반경 내에 위치한 샘플</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_samples</span></code>: <span class="math notranslate nohighlight">\(\varepsilon\)</span> 반경 내에 위치하는 이웃의 수</p></li>
</ul>
</li>
</ul>
<p><strong>핵심샘플과 군집</strong></p>
<ul class="simple">
<li><p>핵심샘플: <span class="math notranslate nohighlight">\(\varepsilon\)</span> 반경 내에 자신을 포함해서 <code class="docutils literal notranslate"><span class="pre">min-samples</span></code>개의 이웃을 갖는 샘플</p></li>
<li><p>군집: 핵심샘플로 이루어진 이웃들로 구성된 그룹</p></li>
</ul>
<p><strong>이상치</strong></p>
<ul class="simple">
<li><p>핵심샘플이 아니면서 동시에 핵심샘플의 이웃도 아닌 샘플.</p></li>
</ul>
<p><strong>예제:</strong> 반달모양 데이터 활용</p>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">dbscan</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-16.png" width="600"/></div><p><strong>DBSCAN과 예측</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">predict()</span></code> 메서드 지원하지 않음.</p></li>
<li><p>이유: <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> 등 보다 좋은 성능의 분류 알고리즘 활용 가능.</p></li>
<li><p>아래 코드: 핵심샘플 대상 훈련.</p></li>
</ul>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dbscan</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">labels_</span><span class="p">[</span><span class="n">dbscan</span><span class="o">.</span><span class="n">core_sample_indices_</span><span class="p">])</span>
</pre></div>
</div>
<hr class="docutils" />
<ul class="simple">
<li><p>이후 새로운 샘플에 대한 예측 가능</p></li>
<li><p>아래 그림은 새로운 4개의 샘플에 대한 예측을 보여줌.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-17.png" width="450"/></div><p><strong>이상치 판단</strong></p>
<ul class="simple">
<li><p>위 예제에서, 두 군집으로부터 일정거리 이상 떨어진 샘플을 이상치로 간주 가능.</p></li>
<li><p>예를 들어, 양편 끝쪽에 위치한 두 개의 샘플이 이상치로 간주될 수 있음.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-17a.png" width="450"/></div><p><strong>DBSCAN의 장단점</strong></p>
<ul class="simple">
<li><p>매우 간단하면서 매우 강력한 알고리즘.</p>
<ul>
<li><p>하이퍼파라미터: 단 2개</p></li>
</ul>
</li>
<li><p>군집의 모양과 개수에 상관없음.</p></li>
<li><p>이상치에 안정적임.</p></li>
<li><p>군깁 간의 밀집도가 크게 다르면 모든 군집 파악 불가능.</p></li>
</ul>
<p><strong>계산복잡도</strong></p>
<ul class="simple">
<li><p>시간복잡도: 약 <span class="math notranslate nohighlight">\(O(m\, \log m)\)</span>. 단, <span class="math notranslate nohighlight">\(m\)</span>은 샘플 수</p></li>
<li><p>공간복잡도: 사이킷런의 DBSCAN 모델은 <span class="math notranslate nohighlight">\(O(m^2)\)</span>의 메모리 요구.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>가 커질 경우.</p></li>
</ul>
</li>
</ul>
<p><strong>기타 군집 알고리즘</strong></p>
<ul class="simple">
<li><p>응집 군집(병합 군집, agglomerative clustering)</p></li>
<li><p>BIRCH</p></li>
<li><p>평균-이동</p></li>
<li><p>유사도 전파</p></li>
<li><p>스펙트럼 군집</p></li>
</ul>
</section>
<section id="id11">
<h2><span class="section-number">9.5. </span>가우스 혼합 모델<a class="headerlink" href="#id11" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>데이터셋이 여러 개의 혼합된 가우스 분포를 따르는 샘플들로 구성되었다고 가정.</p></li>
<li><p>가우스 분포 = 정규분포</p></li>
</ul>
<p><strong>정규분포 소개</strong></p>
<ul class="simple">
<li><p>종 모양의 확률밀도함수를 갖는 확률분포</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-18.png" width="400"/></div><p><strong>군집</strong></p>
<ul class="simple">
<li><p>하나의 가우스 분포에서 생생된 모든 샘플들의 그룹</p></li>
<li><p>일반적으로 타원형 모양.</p></li>
</ul>
<p><strong>예제</strong></p>
<ul class="simple">
<li><p>아래 그림에서처럼 일반적으로 모양, 크기, 밀집도, 방향이 다름.</p></li>
<li><p>따라서 각 샘플이 어떤 정규분포를 따르는지를 파악하는 게 핵심.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-13.png" width="600"/></div><p><strong>GMM 활용</strong></p>
<ul class="simple">
<li><p>위 데이터셋에 <code class="docutils literal notranslate"><span class="pre">GaussianMixture</span></code> 모델 적용</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_components</span></code>: 군집수 지정</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_init</span></code>: 모델 학습 반복 횟수.</p>
<ul>
<li><p>파라미터(평균값, 공분산 등)를 무작위로 추정한 후 수렴할 때까지 학습시킴.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<span class="n">gm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<ul class="simple">
<li><p>아래 그림은 학습된 모델을 보여줌.</p>
<ul>
<li><p>군집 평균, 결정 경계, 밀도 등고선</p></li>
</ul>
</li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-19.png" width="500"/></div><p><strong>GMM 모델 규제</strong></p>
<ul class="simple">
<li><p>특성수가 크거나, 군집수가 많거나, 샘플이 적은 경우 최적 모델 학습 어려움.</p></li>
<li><p>공분산(covariance)에 규제를 가해서 학습을 도와줄 수 있음.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">covariance_type</span></code> 설정.</p></li>
</ul>
</li>
</ul>
<p><strong>covariance_type 옵션값</strong></p>
<ul class="simple">
<li><p>full</p>
<ul>
<li><p>아무런 제한 없음.</p></li>
<li><p>기본값임.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>spherical</p>
<ul>
<li><p>군집이 원형이라 가정.</p></li>
<li><p>지름(분산)은 다를 수 있음.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>diag</p>
<ul>
<li><p>어떤 타원형도 가능.</p></li>
<li><p>단. 타원의 축이 좌표축과 평행하다고 가정.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>tied</p>
<ul>
<li><p>모든 군집의 동일 모양, 동일 크기, 동일 방향을 갖는다고 가정.</p></li>
</ul>
</li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-20.png" width="600"/></div><section id="id12">
<h3><span class="section-number">9.5.1. </span>가우스 혼합 모델 활용: 이상치 탐지<a class="headerlink" href="#id12" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>밀도가 임곗값보다 낮은 지역에 있는 샘플을 이상치로 간주 가능.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-21.png" width="500"/></div></section>
<section id="id13">
<h3><span class="section-number">9.5.2. </span>가우션 혼합모델 군집수 지정<a class="headerlink" href="#id13" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>k-평균에서 사용했던 관성 또는 실루엣 점수 사용 불가.</p>
<ul>
<li><p>군집이 타원형일 때 값이 일정하지 않기 때문.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>대신에 <strong>이론적 정보 기준</strong> 을 최소화 하는 모델 선택 가능.</p></li>
</ul>
<p><strong>이론적 정보 기준</strong></p>
<ul>
<li><p>BIC: Bayesian information criterion</p>
<div class="math notranslate nohighlight">
\[ \log(m)\, p - 2 \log (\hat L)\]</div>
</li>
</ul>
<ul>
<li><p>AIC: Akaike information criterion</p>
<div class="math notranslate nohighlight">
\[ 2\, p - 2 \log (\hat L)\]</div>
</li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(m\)</span>: 샘플 수</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span>: 모델이 학습해야 할 파라미터 수</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat L\)</span>: 모델의 가능도 함수의 최댓값</p></li>
</ul>
<ul class="simple">
<li><p>학습해야 할 파라미터가 많을 수록 벌칙이 가해짐.</p></li>
<li><p>데이터에 잘 학습하는 모델일 수록 보상을 더해줌.</p></li>
</ul>
<p><strong>군집수와 정보조건</strong></p>
<ul class="simple">
<li><p>아래 그림은 군집수 <span class="math notranslate nohighlight">\(k\)</span>와 AIC, BIC의 관계를 보여줌.</p></li>
<li><p><span class="math notranslate nohighlight">\(k=3\)</span>이 최적으로 보임.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-22.png" width="600"/></div></section>
<section id="id14">
<h3><span class="section-number">9.5.3. </span>베이즈 가우스 혼합 모델<a class="headerlink" href="#id14" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>베이즈 확률통계론 활용</p></li>
</ul>
<p><strong>BayesianGaussianMixture 모델</strong></p>
<ul class="simple">
<li><p>최적의 군집수를 자동으로 찾아줌.</p></li>
<li><p>단, 최적의 군집수보다 큰 수를 <code class="docutils literal notranslate"><span class="pre">n_components</span></code>에 전달해야 함.</p>
<ul>
<li><p>즉, 군집에 대한 최소한의 정보를 알고 있다고 가정.</p></li>
</ul>
</li>
<li><p>자동으로 불필요한 군집 제거</p></li>
</ul>
<hr class="docutils" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">BayesianGaussianMixture</span>

<span class="n">bgm</span> <span class="o">=</span> <span class="n">BayesianGaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">bgm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<hr class="docutils" />
<ul class="simple">
<li><p>결과는 군집수 3개를 사용한 이전 결과와 거의 동일.</p></li>
<li><p>군집수 확인 가능</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">bgm</span><span class="o">.</span><span class="n">weights_</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])</span>
</pre></div>
</div>
<p><strong>사전 믿음</strong></p>
<ul class="simple">
<li><p>군집수가 어느 정도일까를 나타내는 지수</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_concentration_prior</span></code> 하이퍼파라미터</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">n_components</span></code>에 설정된 군집수에 대한 규제로 사용됨.</p></li>
<li><p>작은 값이면 특정 군집의 가중치를 0에 가깝게 만들어 군집수를 줄이도록 함.</p></li>
<li><p>즉, 큰 값일 수록 <code class="docutils literal notranslate"><span class="pre">n_components</span></code>에 설정된 군집수가 유지되도록 함.</p></li>
</ul>
</li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-24.png" width="600"/></div><p><strong>가우스 혼합 모델의 장단점</strong></p>
<ul class="simple">
<li><p>타원형 군집에 잘 작동.</p></li>
<li><p>하지만 다른 모양을 가진 데이터셋에서는 성능 좋지 않음.</p></li>
<li><p>예제: 달모양 데이터에 적용하는 경우</p>
<ul>
<li><p>억지로 타원을 찾으려 시도함.</p></li>
</ul>
</li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/handson-ml3/master/jupyter-book/imgs/ch09/homl09-23.png" width="700"/></div></section>
<section id="id15">
<h3><span class="section-number">9.5.4. </span>이상치 탐지와 특이치 탐지를 위한 다른 알고리즘<a class="headerlink" href="#id15" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>PCA</p></li>
<li><p>Fast-MCD</p></li>
<li><p>아이솔레이션 포레스트</p></li>
<li><p>LOF</p></li>
<li><p>one-class SVM</p></li>
</ul>
</section>
</section>
<section id="id16">
<h2><span class="section-number">9.6. </span>연습문제<a class="headerlink" href="#id16" title="Permalink to this headline">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://codingalzi.github.io/handson-ml3/end2end_ml_project.html#id18">2장</a>에서 정의한
<code class="docutils literal notranslate"><span class="pre">ClusterSimilarity</span></code>에 사용된 <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> 모델의 <code class="docutils literal notranslate"><span class="pre">n_cluster</span></code> 인자의 최적값을 확인하라.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="dimensionality_reduction.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">8. </span>차원 축소</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 내용
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">9.1. 분류 대 군집화</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k">9.2. k-평균</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">9.2.1. k-평균 알고리즘</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">9.2.2. 센트로이드 초기화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">9.2.3. 최적의 군집수</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">9.2.4. k-평균의 한계</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">9.3. 군집화 활용</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">9.3.1. 이미지 분할</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">9.3.2. 준지도 학습</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">9.3.3. 레이블 전파</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dbscan">9.4. DBSCAN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">9.5. 가우스 혼합 모델</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">9.5.1. 가우스 혼합 모델 활용: 이상치 탐지</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">9.5.2. 가우션 혼합모델 군집수 지정</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">9.5.3. 베이즈 가우스 혼합 모델</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">9.5.4. 이상치 탐지와 특이치 탐지를 위한 다른 알고리즘</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">9.6. 연습문제</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
으로 코딩알지
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>